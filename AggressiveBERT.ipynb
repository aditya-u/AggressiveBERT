{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nmXRra3Y3G_G"
      },
      "outputs": [],
      "source": [
        "import errno\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "def assert_exits(path):\n",
        "    assert os.path.exists(path), 'Does not exist : {}'.format(path)\n",
        "    \n",
        "def equal_info(a,b):\n",
        "    assert len(a)==len(b),'File info not equal!'\n",
        "    \n",
        "def same_question(a,b):\n",
        "    assert a==b,'Not the same question!'\n",
        "    \n",
        "class Logger(object):\n",
        "\tdef __init__(self,output_dir):\n",
        "\t\tdirname=os.path.dirname(output_dir)\n",
        "\t\tif not os.path.exists(dirname):\n",
        "\t\t\tos.mkdir(dirname)\n",
        "\t\tself.log_file=open(output_dir,'w')\n",
        "\t\tself.infos={}\n",
        "\t\t\n",
        "\tdef append(self,key,val):\n",
        "\t\tvals=self.infos.setdefault(key,[])\n",
        "\t\tvals.append(val)\n",
        "\n",
        "\tdef log(self,extra_msg=''):\n",
        "\t\tmsgs=[extra_msg]\n",
        "\t\tfor key, vals in self.infos.iteritems():\n",
        "\t\t\tmsgs.append('%s %.6f' %(key,np.mean(vals)))\n",
        "\t\tmsg='\\n'.joint(msgs)\n",
        "\t\tself.log_file.write(msg+'\\n')\n",
        "\t\tself.log_file.flush()\n",
        "\t\tself.infos={}\n",
        "\t\treturn msg\n",
        "\t\t\n",
        "\tdef write(self,msg):\n",
        "\t\tself.log_file.write(msg+'\\n')\n",
        "\t\tself.log_file.flush()\n",
        "\t\tprint(msg)\n",
        "\n",
        "class NpEncoder(json.JSONEncoder):\n",
        "    def default(self, obj):\n",
        "        if isinstance(obj, np.integer):\n",
        "            return int(obj)\n",
        "        elif isinstance(obj, np.floating):\n",
        "            return float(obj)\n",
        "        elif isinstance(obj, np.ndarray):\n",
        "            return obj.tolist()\n",
        "        else:\n",
        "            return super(NpEncoder, self).default(obj)\n",
        "\n",
        "\n",
        "def attention_dump(opt,model,test_set, idx):\n",
        "    # print (\"The information for task {} # iterations : {} last batch : {}\".format(source, len(test_set),test_set.last_batch))\n",
        "    tasks=opt.TASKS.split(',')\n",
        "    total = len(test_set)\n",
        "    words_list = []\n",
        "\n",
        "    weights_info = {\"0\":[],\"1\":[],\"2\":[],\"3\":[]}\n",
        "    preds_info = {\"0\":[],\"1\":[],\"2\":[],\"3\":[]}\n",
        "    \n",
        "    for i in range(total):\n",
        "        with torch.no_grad():\n",
        "            batch_info=test_set.next_batch()\n",
        "            tokens=batch_info['tokens'].cuda()\n",
        "            labels=batch_info['label'].float().cuda()\n",
        "            bert_tokens=batch_info['bert_tokens'].cuda()\n",
        "            masks=batch_info['masks'].cuda()\n",
        "            att_masks = batch_info['att_masks'].cuda()\n",
        "            words=batch_info['words']\n",
        "            words_list.extend(words)\n",
        "            for task_idx in range(len(tasks)):\n",
        "                pred,weights=model(tokens,task_idx,bert_tokens,masks,att_masks)\n",
        "                weights_info[str(task_idx)].extend(weights.cpu().numpy())\n",
        "                preds_info[str(task_idx)].extend(pred.cpu().numpy())\n",
        "        if i==0:\n",
        "            #print ('yes')\n",
        "            t1_labels=labels\n",
        "        else:\n",
        "            t1_labels=torch.cat((t1_labels,labels),0)\n",
        "    t1_labels = t1_labels.cpu().numpy()\n",
        "    write_json(words_list, weights_info, preds_info, t1_labels, tasks[0],idx)\n",
        "\n",
        "\n",
        "def write_json(sent_tokens, weights, preds, labels, source,cv_idx):\n",
        "    items = []\n",
        "    for idx in range(len(sent_tokens)):\n",
        "        weights_tuple = (weights[\"0\"][idx],weights[\"1\"][idx],weights[\"2\"][idx],weights[\"3\"][idx])\n",
        "        preds_tuple = (preds[\"0\"][idx],preds[\"1\"][idx],preds[\"2\"][idx],preds[\"3\"][idx])\n",
        "        item = {\"words\": sent_tokens[idx], \"weights\": weights_tuple, \"predictions\": preds_tuple, \"label\": np.argmax(labels[idx])}\n",
        "        items.append(item)\n",
        "\n",
        "    with open(\"weights/{}.attentions.json\".format(source+str(cv_idx)), \"w\") as f:\n",
        "        json.dump(items, f, cls=NpEncoder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1EA0z2B-4TQi",
        "outputId": "d1503d48-90bd-4235-b75c-3fa5bebd5927"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  254M    0  254M    0     0  5190k      0 --:--:--  0:00:50 --:--:-- 7311k\n"
          ]
        }
      ],
      "source": [
        "!curl 'https://gitlab.com/bottle_shop/safe/angrybert/-/archive/master/angrybert-master.zip' --output 'repo.zip'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T3myksHJ5Kgw",
        "outputId": "3b5b8df7-8f04-43d6-89a9-0826ee1d962c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  989M  100  989M    0     0  37.4M      0  0:00:26  0:00:26 --:--:-- 39.3M\n"
          ]
        }
      ],
      "source": [
        "!curl 'https://ia803006.us.archive.org/1/items/glove.6B.50d-300d/glove.6B.300d.txt' --output 'glove.6B.100d.txt'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iwnqSIQ-4pWM",
        "outputId": "176afd63-ce32-4413-c059-5e3e0193f258"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/repo.zip\n",
            "4812983eace58fd7e2f3b5dce115e3967dea4ba4\n",
            "   creating: angrybert-master/\n",
            " extracting: angrybert-master/.gitignore  \n",
            "   creating: angrybert-master/Bert-MTL/\n",
            "   creating: angrybert-master/Bert-MTL/.ipynb_checkpoints/\n",
            "  inflating: angrybert-master/Bert-MTL/.ipynb_checkpoints/samples-checkpoint.ipynb  \n",
            "  inflating: angrybert-master/Bert-MTL/attention.py  \n",
            "  inflating: angrybert-master/Bert-MTL/baseline.py  \n",
            "  inflating: angrybert-master/Bert-MTL/classifier.py  \n",
            "  inflating: angrybert-master/Bert-MTL/config.py  \n",
            "  inflating: angrybert-master/Bert-MTL/dataset.py  \n",
            "   creating: angrybert-master/Bert-MTL/dictionary/\n",
            "  inflating: angrybert-master/Bert-MTL/dictionary/dictionary.pkl  \n",
            "  inflating: angrybert-master/Bert-MTL/dictionary/glove_embedding.npy  \n",
            "   creating: angrybert-master/Bert-MTL/dt/\n",
            "  inflating: angrybert-master/Bert-MTL/dt/dictionary.pkl  \n",
            "  inflating: angrybert-master/Bert-MTL/dt/final_0.txt  \n",
            "  inflating: angrybert-master/Bert-MTL/dt/final_1.txt  \n",
            "  inflating: angrybert-master/Bert-MTL/dt/final_2.txt  \n",
            "  inflating: angrybert-master/Bert-MTL/dt/final_3.txt  \n",
            "  inflating: angrybert-master/Bert-MTL/dt/final_4.txt  \n",
            "  inflating: angrybert-master/Bert-MTL/dt/final_5.txt  \n",
            "  inflating: angrybert-master/Bert-MTL/dt/glove_embedding.npy  \n",
            "  inflating: angrybert-master/Bert-MTL/dt/log0.txt  \n",
            "  inflating: angrybert-master/Bert-MTL/dt/log1.txt  \n",
            "  inflating: angrybert-master/Bert-MTL/dt/log2.txt  \n",
            "  inflating: angrybert-master/Bert-MTL/dt/log3.txt  \n",
            "  inflating: angrybert-master/Bert-MTL/dt/log4.txt  \n",
            "  inflating: angrybert-master/Bert-MTL/dt/log5.txt  \n",
            "  inflating: angrybert-master/Bert-MTL/fc.py  \n",
            "   creating: angrybert-master/Bert-MTL/founta/\n",
            "  inflating: angrybert-master/Bert-MTL/founta/dictionary.pkl  \n",
            "  inflating: angrybert-master/Bert-MTL/founta/final_0.txt  \n",
            "  inflating: angrybert-master/Bert-MTL/founta/final_1.txt  \n",
            "  inflating: angrybert-master/Bert-MTL/founta/final_2.txt  \n",
            "  inflating: angrybert-master/Bert-MTL/founta/final_3.txt  \n",
            "  inflating: angrybert-master/Bert-MTL/founta/final_4.txt  \n",
            "  inflating: angrybert-master/Bert-MTL/founta/final_5.txt  \n",
            "  inflating: angrybert-master/Bert-MTL/founta/glove_embedding.npy  \n",
            "  inflating: angrybert-master/Bert-MTL/founta/log0.txt  \n",
            "  inflating: angrybert-master/Bert-MTL/founta/log1.txt  \n",
            "  inflating: angrybert-master/Bert-MTL/founta/log2.txt  \n",
            "  inflating: angrybert-master/Bert-MTL/founta/log3.txt  \n",
            "  inflating: angrybert-master/Bert-MTL/founta/log4.txt  \n",
            "  inflating: angrybert-master/Bert-MTL/founta/log5.txt  \n",
            "  inflating: angrybert-master/Bert-MTL/full_rnn.py  \n",
            "  inflating: angrybert-master/Bert-MTL/language_model.py  \n",
            "  inflating: angrybert-master/Bert-MTL/main.py  \n",
            "  inflating: angrybert-master/Bert-MTL/preprocessing.py  \n",
            "  inflating: angrybert-master/Bert-MTL/run.sh  \n",
            "  inflating: angrybert-master/Bert-MTL/samples.ipynb  \n",
            "  inflating: angrybert-master/Bert-MTL/train.py  \n",
            "  inflating: angrybert-master/Bert-MTL/train_old.py  \n",
            "  inflating: angrybert-master/Bert-MTL/utils.py  \n",
            "   creating: angrybert-master/Bert-MTL/weights/\n",
            "  inflating: angrybert-master/Bert-MTL/weights/dt.analysis.json  \n",
            "  inflating: angrybert-master/Bert-MTL/weights/dt.attentions.json  \n",
            "  inflating: angrybert-master/Bert-MTL/weights/dt0.analysis.json  \n",
            "  inflating: angrybert-master/Bert-MTL/weights/dt0.attentions.json  \n",
            "  inflating: angrybert-master/Bert-MTL/weights/dt0.mtddn.analysis.json  \n",
            "  inflating: angrybert-master/Bert-MTL/weights/dt1.analysis.json  \n",
            "  inflating: angrybert-master/Bert-MTL/weights/dt1.attentions.json  \n",
            "  inflating: angrybert-master/Bert-MTL/weights/dt1.mtddn.analysis.json  \n",
            "  inflating: angrybert-master/Bert-MTL/weights/dt2.analysis.json  \n",
            "  inflating: angrybert-master/Bert-MTL/weights/dt2.attentions.json  \n",
            "  inflating: angrybert-master/Bert-MTL/weights/dt2.mtddn.analysis.json  \n",
            "  inflating: angrybert-master/Bert-MTL/weights/dt3.analysis.json  \n",
            "  inflating: angrybert-master/Bert-MTL/weights/dt3.attentions.json  \n",
            "  inflating: angrybert-master/Bert-MTL/weights/dt3.mtddn.analysis.json  \n",
            "  inflating: angrybert-master/Bert-MTL/weights/dt4.analysis.json  \n",
            "  inflating: angrybert-master/Bert-MTL/weights/dt4.attentions.json  \n",
            "  inflating: angrybert-master/Bert-MTL/weights/dt4.mtddn.analysis.json  \n",
            "  inflating: angrybert-master/Bert-MTL/weights/founta.analysis.json  \n",
            "  inflating: angrybert-master/Bert-MTL/weights/founta.attentions.json  \n",
            "  inflating: angrybert-master/Bert-MTL/weights/founta0.analysis.json  \n",
            "  inflating: angrybert-master/Bert-MTL/weights/founta0.attentions.json  \n",
            "  inflating: angrybert-master/Bert-MTL/weights/founta1.analysis.json  \n",
            "  inflating: angrybert-master/Bert-MTL/weights/founta1.attentions.json  \n",
            "  inflating: angrybert-master/Bert-MTL/weights/founta2.analysis.json  \n",
            "  inflating: angrybert-master/Bert-MTL/weights/founta2.attentions.json  \n",
            "  inflating: angrybert-master/Bert-MTL/weights/founta3.analysis.json  \n",
            "  inflating: angrybert-master/Bert-MTL/weights/founta3.attentions.json  \n",
            "  inflating: angrybert-master/Bert-MTL/weights/founta4.analysis.json  \n",
            "  inflating: angrybert-master/Bert-MTL/weights/founta4.attentions.json  \n",
            "  inflating: angrybert-master/Bert-MTL/weights/wz.analysis.json  \n",
            "  inflating: angrybert-master/Bert-MTL/weights/wz.attentions.json  \n",
            "  inflating: angrybert-master/Bert-MTL/weights/wz0.analysis.json  \n",
            "  inflating: angrybert-master/Bert-MTL/weights/wz0.attentions.json  \n",
            "  inflating: angrybert-master/Bert-MTL/weights/wz1.analysis.json  \n",
            "  inflating: angrybert-master/Bert-MTL/weights/wz1.attentions.json  \n",
            "  inflating: angrybert-master/Bert-MTL/weights/wz2.analysis.json  \n",
            "  inflating: angrybert-master/Bert-MTL/weights/wz2.attentions.json  \n",
            "  inflating: angrybert-master/Bert-MTL/weights/wz3.analysis.json  \n",
            "  inflating: angrybert-master/Bert-MTL/weights/wz3.attentions.json  \n",
            "  inflating: angrybert-master/Bert-MTL/weights/wz4.analysis.json  \n",
            "  inflating: angrybert-master/Bert-MTL/weights/wz4.attentions.json  \n",
            "   creating: angrybert-master/Bert-MTL/wz/\n",
            "  inflating: angrybert-master/Bert-MTL/wz/dictionary.pkl  \n",
            "  inflating: angrybert-master/Bert-MTL/wz/final_0.txt  \n",
            " extracting: angrybert-master/Bert-MTL/wz/final_1.txt  \n",
            "  inflating: angrybert-master/Bert-MTL/wz/final_2.txt  \n",
            "  inflating: angrybert-master/Bert-MTL/wz/final_3.txt  \n",
            " extracting: angrybert-master/Bert-MTL/wz/final_4.txt  \n",
            " extracting: angrybert-master/Bert-MTL/wz/final_5.txt  \n",
            " extracting: angrybert-master/Bert-MTL/wz/final_6.txt  \n",
            " extracting: angrybert-master/Bert-MTL/wz/final_7.txt  \n",
            " extracting: angrybert-master/Bert-MTL/wz/final_8.txt  \n",
            " extracting: angrybert-master/Bert-MTL/wz/final_9.txt  \n",
            "  inflating: angrybert-master/Bert-MTL/wz/glove_embedding.npy  \n",
            "  inflating: angrybert-master/Bert-MTL/wz/log0.txt  \n",
            "  inflating: angrybert-master/Bert-MTL/wz/log1.txt  \n",
            "  inflating: angrybert-master/Bert-MTL/wz/log2.txt  \n",
            "  inflating: angrybert-master/Bert-MTL/wz/log3.txt  \n",
            "  inflating: angrybert-master/Bert-MTL/wz/log4.txt  \n",
            "  inflating: angrybert-master/Bert-MTL/wz/log5.txt  \n",
            "  inflating: angrybert-master/Bert-MTL/wz/log6.txt  \n",
            "  inflating: angrybert-master/Bert-MTL/wz/log7.txt  \n",
            "  inflating: angrybert-master/Bert-MTL/wz/log8.txt  \n",
            "  inflating: angrybert-master/Bert-MTL/wz/log9.txt  \n",
            "  inflating: angrybert-master/Readings.md  \n",
            "   creating: angrybert-master/generate_data/\n",
            "   creating: angrybert-master/generate_data/__pycache__/\n",
            "  inflating: angrybert-master/generate_data/__pycache__/preprocessing.cpython-36.pyc  \n",
            "   creating: angrybert-master/generate_data/data/\n",
            "  inflating: angrybert-master/generate_data/data/dt.pkl  \n",
            "  inflating: angrybert-master/generate_data/data/emotion.pkl  \n",
            "  inflating: angrybert-master/generate_data/data/founta.pkl  \n",
            "  inflating: angrybert-master/generate_data/data/hatelingo.pkl  \n",
            "  inflating: angrybert-master/generate_data/data/semeval_a.pkl  \n",
            "  inflating: angrybert-master/generate_data/data/semeval_b.pkl  \n",
            "  inflating: angrybert-master/generate_data/data/semeval_c.pkl  \n",
            "  inflating: angrybert-master/generate_data/data/wz.pkl  \n",
            "  inflating: angrybert-master/generate_data/generate_data.py  \n",
            "  inflating: angrybert-master/generate_data/preprocessing.py  \n",
            "  inflating: angrybert-master/generate_data/re_gen.py  \n",
            "  inflating: angrybert-master/generate_data/run.sh  \n",
            "  inflating: angrybert-master/readme.md  \n",
            " extracting: angrybert-master/requirements.txt  \n",
            "   creating: angrybert-master/resource/\n",
            "  inflating: angrybert-master/resource/dt.pkl  \n",
            "  inflating: angrybert-master/resource/founta.pkl  \n",
            "  inflating: angrybert-master/resource/hatelingo.pkl  \n",
            "  inflating: angrybert-master/resource/offenseval_c.pkl  \n",
            "  inflating: angrybert-master/resource/semeval_a.pkl  \n",
            "  inflating: angrybert-master/resource/wz.pkl  \n",
            "  inflating: angrybert-master/split_dataset.ipynb  \n",
            "   creating: angrybert-master/targets/\n",
            "  inflating: angrybert-master/targets/is_about_class.json  \n",
            "  inflating: angrybert-master/targets/is_about_disability.json  \n",
            "  inflating: angrybert-master/targets/is_about_ethnicity.json  \n",
            "  inflating: angrybert-master/targets/is_about_gender.json  \n",
            "  inflating: angrybert-master/targets/is_about_nationality.json  \n",
            "  inflating: angrybert-master/targets/is_about_religion.json  \n",
            "  inflating: angrybert-master/targets/is_about_sexual_orientation.json  \n",
            "  inflating: angrybert-master/targets/target_freq_five.pkl  \n",
            "  inflating: angrybert-master/targets/target_words_v1.txt  \n"
          ]
        }
      ],
      "source": [
        "!unzip '/content/repo.zip'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gAJSjKwY2t3k"
      },
      "outputs": [],
      "source": [
        "import argparse \n",
        "\n",
        "def parse_opt():\n",
        "    parser=argparse.ArgumentParser()\n",
        "    \n",
        "    \"\"\"\n",
        "    basic: shared and private rnns, concatenation of both to the fc layer\n",
        "    dnn: shared and private embeddings, private rnn (mt-dnn similar)\n",
        "    cnn: shared and private embeddings, private cnn\n",
        "    uniform: different embeddings, shared rnn, 2016 IJCAI first baseline\n",
        "    local: 2016 IJCAI, shared layer in the paper\n",
        "    sp-mtl: 2016 IJCAI, for the implementation of global fusion\n",
        "    mtl-gatedencoder: joint modeling network, Rajamanickam et al. 20\n",
        "    shared-bert: bert baseline model for multitask shared learning\n",
        "    angrybert: AngryBERT model, this is our proposed model\n",
        "    angrybert-attn: AngryBERT model with attention on top, visualization purpose\n",
        "    \"\"\"\n",
        "    \n",
        "    parser.add_argument('--MODEL',type=str,default='angrybert')\n",
        "    \n",
        "    '''path configuration'''\n",
        "    parser.add_argument('--GLOVE_PATH',type=str,default='/content/glove.6B.300d.txt')\n",
        "    #path for pre-precessing and result saving\n",
        "    parser.add_argument('--DT',type=str,default='./dt')\n",
        "    parser.add_argument('--WZ_RESULT',type=str,default='./wz')\n",
        "    parser.add_argument('--FOUNTA_RESULT',type=str,default='./founta')\n",
        "    parser.add_argument('--HATELINGO_RESULT',type=str,default='./hatelingo')\n",
        "    parser.add_argument('--DICT_INFO',type=str,default='./dictionary')\n",
        "    \n",
        "    #path for the split dataset\n",
        "    parser.add_argument('--SPLIT_DATASET',type=str,default='/content/angrybert-master/resource')\n",
        "    \n",
        "    \n",
        "    '''hyper parameters configuration'''\n",
        "    parser.add_argument('--EMB_DROPOUT',type=float,default=0.5)\n",
        "    parser.add_argument('--FC_DROPOUT',type=float,default=0.2) \n",
        "    parser.add_argument('--MIN_OCC',type=int,default=3)\n",
        "    parser.add_argument('--BATCH_SIZE',type=int,default=60)\n",
        "    parser.add_argument('--EMB_DIM',type=int,default=300)\n",
        "    parser.add_argument('--MID_DIM',type=int,default=256)\n",
        "    parser.add_argument('--PROJ_DIM',type=int,default=32)\n",
        "    parser.add_argument('--NUM_HIDDEN',type=int,default=128)\n",
        "    parser.add_argument('--NUM_FILTER',type=int,default=150)\n",
        "    parser.add_argument('--FILTER_SIZE',type=str,default=\"2,3,4\")\n",
        "    parser.add_argument('--NUM_LAYER',type=int,default=1)\n",
        "    parser.add_argument('--BIDIRECT',type=bool,default=True)\n",
        "    parser.add_argument('--L_RNN_DROPOUT',type=float,default=0.1) \n",
        "    \"\"\"\n",
        "    wz, dt, founta, hatelingo, offenseval_c, semeval_a\n",
        "    it is a str and the first position of the str is the hate speech detection dataset\n",
        "    \"\"\"\n",
        "    parser.add_argument('--TASKS',type=str,default='hatelingo,offenseval_c')\n",
        "    parser.add_argument('--LENGTH',type=int,default=30)\n",
        "    \n",
        "    parser.add_argument('--CREATE_DICT',type=bool,default=True)\n",
        "    parser.add_argument('--CREATE_EMB',type=bool,default=True)\n",
        "    parser.add_argument('--SAVE_NUM',type=int,default=1)\n",
        "    parser.add_argument('--EPOCHS',type=int,default=6)\n",
        "    parser.add_argument('--CROSS_VAL',type=int,default=5)\n",
        "    \n",
        "    parser.add_argument('--SEED', type=int, default=1112, help='random seed')\n",
        "    parser.add_argument('--CUDA_DEVICE', type=int, default=0)\n",
        "    \n",
        "    \n",
        "    parser.add_argument('--EVAL_ITERS', type=int, default=180)\n",
        "    parser.add_argument('--TOTAL_ITERS', type=int, default=1800)\n",
        "    \n",
        "    args=parser.parse_args(args = [])\n",
        "    return args"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "df2UO0SJ8Zcl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fcd35032-3d07-4f4f-d52f-c2b5338345b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33m\r0% [Working]\u001b[0m\r            \rHit:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "\u001b[33m\r0% [Connecting to archive.ubuntu.com] [Connecting to security.ubuntu.com (185.1\u001b[0m\u001b[33m\r0% [1 InRelease gpgv 1,581 B] [Connecting to archive.ubuntu.com (185.125.190.39\u001b[0m\r                                                                               \rGet:2 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n",
            "\u001b[33m\r0% [1 InRelease gpgv 1,581 B] [Connecting to archive.ubuntu.com (185.125.190.39\u001b[0m\r                                                                               \rGet:3 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "\u001b[33m\r0% [1 InRelease gpgv 1,581 B] [Waiting for headers] [3 InRelease 14.2 kB/88.7 k\u001b[0m\r                                                                               \rGet:4 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\n",
            "\u001b[33m\r0% [1 InRelease gpgv 1,581 B] [Waiting for headers] [3 InRelease 14.2 kB/88.7 k\u001b[0m\r                                                                               \rIgn:5 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:6 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Get:8 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Hit:9 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Get:10 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ Packages [87.8 kB]\n",
            "Hit:11 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",
            "Get:12 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Hit:14 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Get:15 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [1,063 kB]\n",
            "Get:16 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,527 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,306 kB]\n",
            "Get:18 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2,905 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [3,336 kB]\n",
            "Fetched 11.5 MB in 3s (3,969 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "55 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following additional packages will be installed:\n",
            "  aspell aspell-en dictionaries-common emacsen-common hunspell-en-us\n",
            "  libaspell15 libenchant1c2a libhunspell-1.6-0 libtext-iconv-perl\n",
            "Suggested packages:\n",
            "  aspell-doc spellutils wordlist hunspell openoffice.org-hunspell\n",
            "  | openoffice.org-core libenchant-voikko\n",
            "The following NEW packages will be installed:\n",
            "  aspell aspell-en dictionaries-common emacsen-common enchant hunspell-en-us\n",
            "  libaspell15 libenchant1c2a libhunspell-1.6-0 libtext-iconv-perl\n",
            "0 upgraded, 10 newly installed, 0 to remove and 55 not upgraded.\n",
            "Need to get 1,312 kB of archives.\n",
            "After this operation, 5,353 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/main amd64 libtext-iconv-perl amd64 1.7-5build6 [13.0 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libaspell15 amd64 0.60.7~20110707-4ubuntu0.2 [310 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic/main amd64 emacsen-common all 2.0.8 [17.6 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic/main amd64 dictionaries-common all 1.27.2 [186 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 aspell amd64 0.60.7~20110707-4ubuntu0.2 [87.7 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu bionic/main amd64 aspell-en all 2017.08.24-0-0.1 [298 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu bionic/main amd64 hunspell-en-us all 1:2017.08.24 [168 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu bionic/main amd64 libhunspell-1.6-0 amd64 1.6.2-1 [154 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu bionic/main amd64 libenchant1c2a amd64 1.6.0-11.1 [64.4 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu bionic/main amd64 enchant amd64 1.6.0-11.1 [12.2 kB]\n",
            "Fetched 1,312 kB in 1s (1,359 kB/s)\n",
            "Preconfiguring packages ...\n",
            "Selecting previously unselected package libtext-iconv-perl.\n",
            "(Reading database ... 155673 files and directories currently installed.)\n",
            "Preparing to unpack .../0-libtext-iconv-perl_1.7-5build6_amd64.deb ...\n",
            "Unpacking libtext-iconv-perl (1.7-5build6) ...\n",
            "Selecting previously unselected package libaspell15:amd64.\n",
            "Preparing to unpack .../1-libaspell15_0.60.7~20110707-4ubuntu0.2_amd64.deb ...\n",
            "Unpacking libaspell15:amd64 (0.60.7~20110707-4ubuntu0.2) ...\n",
            "Selecting previously unselected package emacsen-common.\n",
            "Preparing to unpack .../2-emacsen-common_2.0.8_all.deb ...\n",
            "Unpacking emacsen-common (2.0.8) ...\n",
            "Selecting previously unselected package dictionaries-common.\n",
            "Preparing to unpack .../3-dictionaries-common_1.27.2_all.deb ...\n",
            "Adding 'diversion of /usr/share/dict/words to /usr/share/dict/words.pre-dictionaries-common by dictionaries-common'\n",
            "Unpacking dictionaries-common (1.27.2) ...\n",
            "Selecting previously unselected package aspell.\n",
            "Preparing to unpack .../4-aspell_0.60.7~20110707-4ubuntu0.2_amd64.deb ...\n",
            "Unpacking aspell (0.60.7~20110707-4ubuntu0.2) ...\n",
            "Selecting previously unselected package aspell-en.\n",
            "Preparing to unpack .../5-aspell-en_2017.08.24-0-0.1_all.deb ...\n",
            "Unpacking aspell-en (2017.08.24-0-0.1) ...\n",
            "Selecting previously unselected package hunspell-en-us.\n",
            "Preparing to unpack .../6-hunspell-en-us_1%3a2017.08.24_all.deb ...\n",
            "Unpacking hunspell-en-us (1:2017.08.24) ...\n",
            "Selecting previously unselected package libhunspell-1.6-0:amd64.\n",
            "Preparing to unpack .../7-libhunspell-1.6-0_1.6.2-1_amd64.deb ...\n",
            "Unpacking libhunspell-1.6-0:amd64 (1.6.2-1) ...\n",
            "Selecting previously unselected package libenchant1c2a:amd64.\n",
            "Preparing to unpack .../8-libenchant1c2a_1.6.0-11.1_amd64.deb ...\n",
            "Unpacking libenchant1c2a:amd64 (1.6.0-11.1) ...\n",
            "Selecting previously unselected package enchant.\n",
            "Preparing to unpack .../9-enchant_1.6.0-11.1_amd64.deb ...\n",
            "Unpacking enchant (1.6.0-11.1) ...\n",
            "Setting up libhunspell-1.6-0:amd64 (1.6.2-1) ...\n",
            "Setting up libaspell15:amd64 (0.60.7~20110707-4ubuntu0.2) ...\n",
            "Setting up emacsen-common (2.0.8) ...\n",
            "Setting up libtext-iconv-perl (1.7-5build6) ...\n",
            "Setting up dictionaries-common (1.27.2) ...\n",
            "Setting up aspell (0.60.7~20110707-4ubuntu0.2) ...\n",
            "Setting up hunspell-en-us (1:2017.08.24) ...\n",
            "Setting up libenchant1c2a:amd64 (1.6.0-11.1) ...\n",
            "Setting up aspell-en (2017.08.24-0-0.1) ...\n",
            "Setting up enchant (1.6.0-11.1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.3) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for dictionaries-common (1.27.2) ...\n",
            "aspell-autobuildhash: processing: en [en-common].\n",
            "aspell-autobuildhash: processing: en [en-variant_0].\n",
            "aspell-autobuildhash: processing: en [en-variant_1].\n",
            "aspell-autobuildhash: processing: en [en-variant_2].\n",
            "aspell-autobuildhash: processing: en [en-w_accents-only].\n",
            "aspell-autobuildhash: processing: en [en-wo_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_AU-variant_0].\n",
            "aspell-autobuildhash: processing: en [en_AU-variant_1].\n",
            "aspell-autobuildhash: processing: en [en_AU-w_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_AU-wo_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_CA-variant_0].\n",
            "aspell-autobuildhash: processing: en [en_CA-variant_1].\n",
            "aspell-autobuildhash: processing: en [en_CA-w_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_CA-wo_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_GB-ise-w_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_GB-ise-wo_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_GB-ize-w_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_GB-ize-wo_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_GB-variant_0].\n",
            "aspell-autobuildhash: processing: en [en_GB-variant_1].\n",
            "aspell-autobuildhash: processing: en [en_US-w_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_US-wo_accents-only].\n",
            "enchant is already the newest version (1.6.0-11.1).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 55 not upgraded.\n"
          ]
        }
      ],
      "source": [
        "!apt update\n",
        "!apt install enchant --fix-missing\n",
        "!apt install -qq enchant"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YQFihuJj53BN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "adf8801c-e27a-480a-f708-b58fab0ca231"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting wordninja\n",
            "  Downloading wordninja-2.0.0.tar.gz (541 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 541 kB 4.3 MB/s \n",
            "\u001b[?25hCollecting compound-word-splitter\n",
            "  Downloading compound-word-splitter-0.4.tar.gz (2.1 kB)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.7)\n",
            "Collecting pyenchant\n",
            "  Downloading pyenchant-3.2.2-py3-none-any.whl (55 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 55 kB 3.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk) (2022.6.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk) (1.1.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk) (4.64.0)\n",
            "Building wheels for collected packages: wordninja, compound-word-splitter\n",
            "  Building wheel for wordninja (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wordninja: filename=wordninja-2.0.0-py3-none-any.whl size=541551 sha256=d711e3c2a755ab43f950406bd0df469b835c3769bb9f3d3fb2998c3a5da5ab56\n",
            "  Stored in directory: /root/.cache/pip/wheels/dd/3f/eb/a2692e3d2b9deb1487b09ba4967dd6920bd5032bfd9ff7acfc\n",
            "  Building wheel for compound-word-splitter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for compound-word-splitter: filename=compound_word_splitter-0.4-py3-none-any.whl size=2565 sha256=46280b877f32c547c86879982d5a61912e6e3013bb11cc98860cbfca8be7f928\n",
            "  Stored in directory: /root/.cache/pip/wheels/d5/9d/f8/68bc335f1ac73027701888d14f4eb9fbd09e0373fd846fa8b1\n",
            "Successfully built wordninja compound-word-splitter\n",
            "Installing collected packages: wordninja, pyenchant, compound-word-splitter\n",
            "Successfully installed compound-word-splitter-0.4 pyenchant-3.2.2 wordninja-2.0.0\n"
          ]
        }
      ],
      "source": [
        "!pip install wordninja compound-word-splitter nltk pyenchant"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0_3JdaH326hw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe084fd8-b2b0-41d5-a6cb-b76f432b853a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "user trying to sneak junior doctors contracts through while media focussed on chilcott you utter cunt fuck you and your tory party\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import pickle\n",
        "import re\n",
        "import string\n",
        "\n",
        "import wordninja\n",
        "import splitter\n",
        "from nltk.tokenize.treebank import TreebankWordTokenizer\n",
        "import numpy as np\n",
        "\n",
        "# Aphost lookup dict\n",
        "symbols_to_delete  = '@ï¼¼ãƒ»Ï‰+=â€â€œ[]^>\\\\Â°<~â€¢â‰ â„¢ËˆÊŠÉ’âˆžÂ§{}Â·Ï„Î±â¤â˜ºÉ¡|Â¢â†’Ì¶`â¥â”â”£â”«â”—ï¼¯â–ºâ˜…Â©â€•Éªâœ”Â®\\x96\\x92â—Â£â™¥âž¤Â´Â¹â˜•â‰ˆÃ·â™¡â—â•‘â–¬â€²É”Ëâ‚¬Û©Ûžâ€ Î¼âœ’âž¥â•â˜†ËŒâ—„Â½Ê»Ï€Î´Î·Î»ÏƒÎµÏÎ½Êƒâœ¬ï¼³ï¼µï¼°ï¼¥ï¼²ï¼©ï¼´â˜»Â±â™ÂµÂºÂ¾âœ“â—¾ØŸï¼Žâ¬…â„…Â»Ð’Ð°Ð²â£â‹…Â¿Â¬â™«ï¼£ï¼­Î²â–ˆâ–“â–’â–‘â‡’â­â€ºÂ¡â‚‚â‚ƒâ§â–°â–”â—žâ–€â–‚â–ƒâ–„â–…â–†â–‡â†™Î³Ì„â€³â˜¹âž¡Â«Ï†â…“â€žâœ‹ï¼šÂ¥Ì²Ì…Ìâˆ™â€›â—‡âœâ–·â“â—Â¶ËšË™ï¼‰ÑÐ¸Ê¿âœ¨ã€‚É‘\\x80â—•ï¼ï¼…Â¯âˆ’ï¬‚ï¬â‚Â²ÊŒÂ¼â´â„â‚„âŒ â™­âœ˜â•ªâ–¶â˜­âœ­â™ªâ˜”â˜ â™‚â˜ƒâ˜ŽâœˆâœŒâœ°â†â˜™â—‹â€£âš“å¹´âˆŽâ„’â–ªâ–™â˜â…›ï½ƒï½ï½“Ç€â„®Â¸ï½—â€šâˆ¼â€–â„³â„â†â˜¼â‹†Ê’âŠ‚ã€â…”Â¨Í¡à¹âš¾âš½Î¦Ã—Î¸ï¿¦ï¼Ÿï¼ˆâ„ƒâ©â˜®âš æœˆâœŠâŒâ­•â–¸â– â‡Œâ˜â˜‘âš¡â˜„Ç«â•­âˆ©â•®ï¼Œä¾‹ï¼žÊ•ÉÌ£Î”â‚€âœžâ”ˆâ•±â•²â–â–•â”ƒâ•°â–Šâ–‹â•¯â”³â”Šâ‰¥â˜’â†‘â˜É¹âœ…â˜›â™©â˜žï¼¡ï¼ªï¼¢â—”â—¡â†“â™€â¬†Ì±â„\\x91â €Ë¤â•šâ†ºâ‡¤âˆâœ¾â—¦â™¬Â³ã®ï½œï¼âˆµâˆ´âˆšÎ©Â¤â˜œâ–²â†³â–«â€¿â¬‡âœ§ï½ï½–ï½ï¼ï¼’ï¼ï¼˜ï¼‡â€°â‰¤âˆ•Ë†âšœâ˜.,?!;*\"â€¦:â€”()%$&/\\nðŸ•\\rðŸµðŸ˜‘\\xa0\\ue014\\t\\uf818\\uf04a\\xadðŸ˜¢ðŸ¶ï¸\\uf0e0ðŸ˜œðŸ˜ŽðŸ‘Š\\u200b\\u200eðŸ˜Ø¹Ø¯ÙˆÙŠÙ‡ØµÙ‚Ø£Ù†Ø§Ø®Ù„Ù‰Ø¨Ù…ØºØ±ðŸ˜ðŸ’–ðŸ’µÐ•ðŸ‘ŽðŸ˜€ðŸ˜‚\\u202a\\u202cðŸ”¥ðŸ˜„ðŸ»ðŸ’¥á´ÊÊ€á´‡É´á´…á´á´€á´‹Êœá´œÊŸá´›á´„á´˜Ê™Ò“á´Šá´¡É¢ðŸ˜‹ðŸ‘×©×œ×•××‘×™ðŸ˜±â€¼\\x81ã‚¨ãƒ³ã‚¸æ•…éšœ\\u2009ðŸšŒá´µÍžðŸŒŸðŸ˜ŠðŸ˜³ðŸ˜§ðŸ™€ðŸ˜ðŸ˜•\\u200fðŸ‘ðŸ˜®ðŸ˜ƒðŸ˜˜××¢×›×—ðŸ’©ðŸ’¯â›½ðŸš„ðŸ¼à®œðŸ˜–á´ ðŸš²â€ðŸ˜ŸðŸ˜ˆðŸ’ªðŸ™ðŸŽ¯ðŸŒ¹ðŸ˜‡ðŸ’”ðŸ˜¡\\x7fðŸ‘Œá¼á½¶Î®Î¹á½²Îºá¼€Î¯á¿ƒá¼´Î¾ðŸ™„ï¼¨ðŸ˜ \\ufeff\\u2028ðŸ˜‰ðŸ˜¤â›ºðŸ™‚\\u3000ØªØ­ÙƒØ³Ø©ðŸ‘®ðŸ’™ÙØ²Ø·ðŸ˜ðŸ¾ðŸŽ‰ðŸ˜ž\\u2008ðŸ¾ðŸ˜…ðŸ˜­ðŸ‘»ðŸ˜¥ðŸ˜”ðŸ˜“ðŸ½ðŸŽ†ðŸ»ðŸ½ðŸŽ¶ðŸŒºðŸ¤”ðŸ˜ª\\x08â€‘ðŸ°ðŸ‡ðŸ±ðŸ™†ðŸ˜¨ðŸ™ƒðŸ’•ð˜Šð˜¦ð˜³ð˜¢ð˜µð˜°ð˜¤ð˜ºð˜´ð˜ªð˜§ð˜®ð˜£ðŸ’—ðŸ’šåœ°ç„è°·ÑƒÐ»ÐºÐ½ÐŸÐ¾ÐÐðŸ¾ðŸ•ðŸ˜†×”ðŸ”—ðŸš½æ­Œèˆžä¼ŽðŸ™ˆðŸ˜´ðŸ¿ðŸ¤—ðŸ‡ºðŸ‡¸Ð¼Ï…Ñ‚Ñ•â¤µðŸ†ðŸŽƒðŸ˜©\\u200aðŸŒ ðŸŸðŸ’«ðŸ’°ðŸ’ŽÑÐ¿Ñ€Ð´\\x95ðŸ–ðŸ™…â›²ðŸ°ðŸ¤ðŸ‘†ðŸ™Œ\\u2002ðŸ’›ðŸ™ðŸ‘€ðŸ™ŠðŸ™‰\\u2004Ë¢áµ’Ê³Ê¸á´¼á´·á´ºÊ·áµ—Ê°áµ‰áµ˜\\x13ðŸš¬ðŸ¤“\\ue602ðŸ˜µÎ¬Î¿ÏŒÏ‚Î­á½¸×ª×ž×“×£× ×¨×š×¦×˜ðŸ˜’ÍðŸ†•ðŸ‘…ðŸ‘¥ðŸ‘„ðŸ”„ðŸ”¤ðŸ‘‰ðŸ‘¤ðŸ‘¶ðŸ‘²ðŸ”›ðŸŽ“\\uf0b7\\uf04c\\x9f\\x10æˆéƒ½ðŸ˜£âºðŸ˜ŒðŸ¤‘ðŸŒðŸ˜¯ÐµÑ…ðŸ˜²á¼¸á¾¶á½ðŸ’žðŸš“ðŸ””ðŸ“šðŸ€ðŸ‘\\u202dðŸ’¤ðŸ‡\\ue613å°åœŸè±†ðŸ¡â”â‰\\u202fðŸ‘ ã€‹à¤•à¤°à¥à¤®à¤¾ðŸ‡¹ðŸ‡¼ðŸŒ¸è”¡è‹±æ–‡ðŸŒžðŸŽ²ãƒ¬ã‚¯ã‚µã‚¹ðŸ˜›å¤–å›½äººå…³ç³»Ð¡Ð±ðŸ’‹ðŸ’€ðŸŽ„ðŸ’œðŸ¤¢ÙÙŽÑŒÑ‹Ð³Ñä¸æ˜¯\\x9c\\x9dðŸ—‘\\u2005ðŸ’ƒðŸ“£ðŸ‘¿à¼¼ã¤à¼½ðŸ˜°á¸·Ð—Ð·â–±Ñ†ï¿¼ðŸ¤£å–æ¸©å“¥åŽè®®ä¼šä¸‹é™ä½ å¤±åŽ»æ‰€æœ‰çš„é’±åŠ æ‹¿å¤§åç¨Žéª—å­ðŸãƒ„ðŸŽ…\\x85ðŸºØ¢Ø¥Ø´Ø¡ðŸŽµðŸŒŽÍŸá¼”æ²¹åˆ«å…‹ðŸ¤¡ðŸ¤¥ðŸ˜¬ðŸ¤§Ð¹\\u2003ðŸš€ðŸ¤´Ê²ÑˆÑ‡Ð˜ÐžÐ Ð¤Ð”Ð¯ÐœÑŽÐ¶ðŸ˜ðŸ–‘á½á½»Ïç‰¹æ®Šä½œæˆ¦ç¾¤Ñ‰ðŸ’¨åœ†æ˜Žå›­×§â„ðŸˆðŸ˜ºðŸŒâá»‡ðŸ”ðŸ®ðŸðŸ†ðŸ‘ðŸŒ®ðŸŒ¯ðŸ¤¦\\u200dð“’ð“²ð“¿ð“µì•ˆì˜í•˜ì„¸ìš”Ð–Ñ™ÐšÑ›ðŸ€ðŸ˜«ðŸ¤¤á¿¦æˆ‘å‡ºç”Ÿåœ¨äº†å¯ä»¥è¯´æ™®é€šè¯æ±‰è¯­å¥½æžðŸŽ¼ðŸ•ºðŸ¸ðŸ¥‚ðŸ—½ðŸŽ‡ðŸŽŠðŸ†˜ðŸ¤ ðŸ‘©ðŸ–’ðŸšªå¤©ä¸€å®¶âš²\\u2006âš­âš†â¬­â¬¯â–æ–°âœ€â•ŒðŸ‡«ðŸ‡·ðŸ‡©ðŸ‡ªðŸ‡®ðŸ‡¬ðŸ‡§ðŸ˜·ðŸ‡¨ðŸ‡¦Ð¥Ð¨ðŸŒ\\x1fæ€é¸¡ç»™çŒ´çœ‹Êð—ªð—µð—²ð—»ð˜†ð—¼ð˜‚ð—¿ð—®ð—¹ð—¶ð˜‡ð—¯ð˜ð—°ð˜€ð˜…ð—½ð˜„ð—±ðŸ“ºÏ–\\u2000Ò¯Õ½á´¦áŽ¥Ò»Íº\\u2007Õ°\\u2001É©ï½™ï½…àµ¦ï½ŒÆ½ï½ˆð“ð¡ðžð«ð®ððšðƒðœð©ð­ð¢ð¨ð§Æ„á´¨×Ÿá‘¯à»Î¤á§à¯¦Ð†á´‘Üð¬ð°ð²ð›ð¦ð¯ð‘ð™ð£ð‡ð‚ð˜ðŸŽÔœÐ¢á—žà±¦ã€”áŽ«ð³ð”ð±ðŸ”ðŸ“ð…ðŸ‹ï¬ƒðŸ’˜ðŸ’“Ñ‘ð˜¥ð˜¯ð˜¶ðŸ’ðŸŒ‹ðŸŒ„ðŸŒ…ð™¬ð™–ð™¨ð™¤ð™£ð™¡ð™®ð™˜ð™ ð™šð™™ð™œð™§ð™¥ð™©ð™ªð™—ð™žð™ð™›ðŸ‘ºðŸ·â„‹ð€ð¥ðªðŸš¶ð™¢á¼¹ðŸ¤˜Í¦ðŸ’¸Ø¬íŒ¨í‹°ï¼·ð™‡áµ»ðŸ‘‚ðŸ‘ƒÉœðŸŽ«\\uf0a7Ð‘Ð£Ñ–ðŸš¢ðŸš‚àª—à«àªœàª°àª¾àª¤à«€á¿†ðŸƒð“¬ð“»ð“´ð“®ð“½ð“¼â˜˜ï´¾Ì¯ï´¿â‚½\\ue807ð‘»ð’†ð’ð’•ð’‰ð’“ð’–ð’‚ð’ð’…ð’”ð’Žð’—ð’ŠðŸ‘½ðŸ˜™\\u200cÐ›â€’ðŸŽ¾ðŸ‘¹âŽŒðŸ’â›¸å…¬å¯“å…»å® ç‰©å—ðŸ„ðŸ€ðŸš‘ðŸ¤·æ“ç¾Žð’‘ð’šð’ð‘´ðŸ¤™ðŸ’æ¬¢è¿Žæ¥åˆ°é˜¿æ‹‰æ–¯×¡×¤ð™«ðŸˆð’Œð™Šð™­ð™†ð™‹ð™ð˜¼ð™…ï·»ðŸ¦„å·¨æ”¶èµ¢å¾—ç™½é¬¼æ„¤æ€’è¦ä¹°é¢áº½ðŸš—ðŸ³ðŸðŸðŸ–ðŸ‘ðŸ•ð’„ðŸ—ð ð™„ð™ƒðŸ‘‡é”Ÿæ–¤æ‹·ð—¢ðŸ³ðŸ±ðŸ¬â¦ãƒžãƒ«ãƒãƒ‹ãƒãƒ­æ ªå¼ç¤¾â›·í•œêµ­ì–´ã„¸ã…“ë‹ˆÍœÊ–ð˜¿ð™”â‚µð’©â„¯ð’¾ð“ð’¶ð“‰ð“‡ð“Šð“ƒð“ˆð“…â„´ð’»ð’½ð“€ð“Œð’¸ð“Žð™Î¶ð™Ÿð˜ƒð—ºðŸ®ðŸ­ðŸ¯ðŸ²ðŸ‘‹ðŸ¦Šå¤šä¼¦ðŸ½ðŸŽ»ðŸŽ¹â›“ðŸ¹ðŸ·ðŸ¦†ä¸ºå’Œä¸­å‹è°Šç¥è´ºä¸Žå…¶æƒ³è±¡å¯¹æ³•å¦‚ç›´æŽ¥é—®ç”¨è‡ªå·±çŒœæœ¬ä¼ æ•™å£«æ²¡ç§¯å”¯è®¤è¯†åŸºç£å¾’æ›¾ç»è®©ç›¸ä¿¡è€¶ç¨£å¤æ´»æ­»æ€ªä»–ä½†å½“ä»¬èŠäº›æ”¿æ²»é¢˜æ—¶å€™æˆ˜èƒœå› åœ£æŠŠå…¨å ‚ç»“å©šå­©ææƒ§ä¸”æ —è°“è¿™æ ·è¿˜â™¾ðŸŽ¸ðŸ¤•ðŸ¤’â›‘ðŸŽæ‰¹åˆ¤æ£€è®¨ðŸðŸ¦ðŸ™‹ðŸ˜¶ì¥ìŠ¤íƒ±íŠ¸ë¤¼ë„ì„ìœ ê°€ê²©ì¸ìƒì´ê²½ì œí™©ì„ë µê²Œë§Œë“¤ì§€ì•Šë¡ìž˜ê´€ë¦¬í•´ì•¼í•©ë‹¤ìºë‚˜ì—ì„œëŒ€ë§ˆì´ˆì™€í™”ì•½ê¸ˆì˜í’ˆëŸ°ì„±ë¶„ê°ˆë•ŒëŠ”ë°˜ë“œì‹œí—ˆëœì‚¬ìš©ðŸ”«ðŸ‘å‡¸á½°ðŸ’²ðŸ—¯ð™ˆá¼Œð’‡ð’ˆð’˜ð’ƒð‘¬ð‘¶ð•¾ð–™ð–—ð–†ð–Žð–Œð–ð–•ð–Šð–”ð–‘ð–‰ð–“ð–ð–œð–žð–šð–‡ð•¿ð–˜ð–„ð–›ð–’ð–‹ð–‚ð•´ð–Ÿð–ˆð•¸ðŸ‘‘ðŸš¿ðŸ’¡çŸ¥å½¼ç™¾\\uf005ð™€ð’›ð‘²ð‘³ð‘¾ð’‹ðŸ’ðŸ˜¦ð™’ð˜¾ð˜½ðŸð˜©ð˜¨á½¼á¹‘ð‘±ð‘¹ð‘«ð‘µð‘ªðŸ‡°ðŸ‡µðŸ‘¾á“‡á’§á”­áƒá§á¦á‘³á¨á“ƒá“‚á‘²á¸á‘­á‘Žá“€á£ðŸ„ðŸŽˆðŸ”¨ðŸŽðŸ¤žðŸ¸ðŸ’ŸðŸŽ°ðŸŒðŸ›³ç‚¹å‡»æŸ¥ç‰ˆðŸ­ð‘¥ð‘¦ð‘§ï¼®ï¼§ðŸ‘£\\uf020ã£ðŸ‰Ñ„ðŸ’­ðŸŽ¥ÎžðŸ´ðŸ‘¨ðŸ¤³ðŸ¦\\x0bðŸ©ð‘¯ð’’ðŸ˜—ðŸðŸ‚ðŸ‘³ðŸ—ðŸ•‰ðŸ²Ú†ÛŒð‘®ð—•ð—´ðŸ’êœ¥â²£â²ðŸ‘â°é‰„ãƒªäº‹ä»¶Ñ—ðŸ’Šã€Œã€\\uf203\\uf09a\\uf222\\ue608\\uf202\\uf099\\uf469\\ue607\\uf410\\ue600ç‡»è£½ã‚·è™šå½å±ç†å±ˆÐ“ð‘©ð‘°ð’€ð‘ºðŸŒ¤ð—³ð—œð—™ð—¦ð—§ðŸŠá½ºá¼ˆá¼¡Ï‡á¿–Î›â¤ðŸ‡³ð’™ÏˆÕÕ´Õ¥Õ¼Õ¡ÕµÕ«Õ¶Ö€Ö‚Õ¤Õ±å†¬è‡³á½€ð’ðŸ”¹ðŸ¤šðŸŽð‘·ðŸ‚ðŸ’…ð˜¬ð˜±ð˜¸ð˜·ð˜ð˜­ð˜“ð˜–ð˜¹ð˜²ð˜«Ú©Î’ÏŽðŸ’¢ÎœÎŸÎÎ‘Î•ðŸ‡±â™²ðˆâ†´ðŸ’’âŠ˜È»ðŸš´ðŸ–•ðŸ–¤ðŸ¥˜ðŸ“ðŸ‘ˆâž•ðŸš«ðŸŽ¨ðŸŒ‘ðŸ»ðŽððŠð‘­ðŸ¤–ðŸŽŽðŸ˜¼ðŸ•·ï½‡ï½’ï½Žï½”ï½‰ï½„ï½•ï½†ï½‚ï½‹ðŸ°ðŸ‡´ðŸ‡­ðŸ‡»ðŸ‡²ð—žð—­ð—˜ð—¤ðŸ‘¼ðŸ“‰ðŸŸðŸ¦ðŸŒˆðŸ”­ã€ŠðŸŠðŸ\\uf10aáƒšÚ¡ðŸ¦\\U0001f92f\\U0001f92aðŸ¡ðŸ’³á¼±ðŸ™‡ð—¸ð—Ÿð— ð—·ðŸ¥œã•ã‚ˆã†ãªã‚‰ðŸ”¼'\n",
        "tokenizer = TreebankWordTokenizer()\n",
        "#isolate_dict = {ord(c): ' {} '.format(c) for c in symbols_to_isolate}\n",
        "remove_dict = {ord(c): '' for c in symbols_to_delete}\n",
        "def handle_contractions(x):\n",
        "    x = tokenizer.tokenize(x)\n",
        "    return x\n",
        "\n",
        "def handle_punctuation(x):\n",
        "    x = x.translate(remove_dict)\n",
        "    #x = x.translate(isolate_dict)\n",
        "    return x\n",
        "\n",
        "def fix_quote(x):\n",
        "    x = [x_[1:] if x_.startswith(\"'\") else x_ for x_ in x]\n",
        "    x = ' '.join(x)\n",
        "    return x\n",
        "\n",
        "def preprocess(x):\n",
        "    x = handle_punctuation(x)\n",
        "    #x = handle_contractions(x)\n",
        "    #x = fix_quote(x)\n",
        "    return x\n",
        "\n",
        "fill = {\"ain't\": \"is not\", \"aren't\": \"are not\", \"can't\": \"cannot\",\n",
        "        \"can't've\": \"cannot have\", \"'cause\": \"because\", \"could've\": \"could have\",\n",
        "        \"couldn't\": \"could not\", \"couldn't've\": \"could not have\", \"didn't\": \"did not\",\n",
        "        \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\",\n",
        "        \"hadn't've\": \"had not have\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n",
        "        \"he'd\": \"he would\", \"he'd've\": \"he would have\", \"he'll\": \"he will\",\n",
        "        \"he'll've\": \"he he will have\", \"he's\": \"he is\", \"how'd\": \"how did\",\n",
        "        \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\n",
        "        \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\",\n",
        "        \"I'll've\": \"I will have\", \"I'm\": \"I am\", \"I've\": \"I have\",\n",
        "        \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",\n",
        "        \"i'll've\": \"i will have\", \"i'm\": \"i am\", \"i've\": \"i have\",\n",
        "        \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\",\n",
        "        \"it'll\": \"it will\", \"it'll've\": \"it will have\", \"it's\": \"it is\",\n",
        "        \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\",\n",
        "        \"might've\": \"might have\", \"mightn't\": \"might not\", \"mightn't've\": \"might not have\",\n",
        "        \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\",\n",
        "        \"needn't\": \"need not\", \"needn't've\": \"need not have\", \"o'clock\": \"of the clock\",\n",
        "        \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\",\n",
        "        \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\",\n",
        "        \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\",\n",
        "        \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\",\n",
        "        \"shouldn't've\": \"should not have\", \"so've\": \"so have\", \"so's\": \"so as\",\n",
        "        \"this's\": \"this is\",\n",
        "        \"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\",\n",
        "        \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\",\n",
        "        \"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\",\n",
        "        \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\",\n",
        "        \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\",\n",
        "        \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\",\n",
        "        \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\",\n",
        "        \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",\n",
        "        \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\",\n",
        "        \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\n",
        "        \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\",\n",
        "        \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\",\n",
        "        \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\",\n",
        "        \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\",\n",
        "        \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\n",
        "        \"y'all'd've\": \"you all would have\", \"y'all're\": \"you all are\", \"y'all've\": \"you all have\",\n",
        "        \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\",\n",
        "        \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\", \"u.s.\": \"united states\",\n",
        "        \"#lol\": \"laughing out loud\", \"#lamo\": \"laughing my ass off\", \"#rof\": \"rolling on the floor laughing\",\n",
        "        \"#covfefe\": \"ironic\", \"wtf\": \"what the fuck\", \"#wtf\": \"what the fuck\",\n",
        "        \"tbh\": \"to be honest\"}\n",
        "\n",
        "slang = {\n",
        "    \"4ward\": \"forward\",\n",
        "    \"brb\": \"be right back\",\n",
        "    \"b4\": \"before\",\n",
        "    \"bfn\": \"bye for now\",\n",
        "    \"bgd\": \"background\",\n",
        "    \"btw\": \"by the way\",\n",
        "    \"br\": \"best regards\",\n",
        "    \"clk\": \"click\",\n",
        "    \"da\": \"the\",\n",
        "    \"deet\": \"detail\",\n",
        "    \"deets\": \"details\",\n",
        "    \"dm\": \"direct message\",\n",
        "    \"f2f\": \"face to face\",\n",
        "    \"ftl\": \" for the loss\",\n",
        "    \"ftw\": \"for the win\",\n",
        "    \"f**k\": \"fuck\",\n",
        "    \"f**ked\": \"fucked\",\n",
        "    \"b***ch\": \"bitch\",\n",
        "    \"kk\": \"cool cool\",\n",
        "    \"kewl\": \"cool\",\n",
        "    \"smh\": \"so much hate\",\n",
        "    \"yaass\": \"yes\",\n",
        "    \"a$$\": \"ass\",\n",
        "    \"bby\": \"baby\",\n",
        "    \"bc\": \"because\",\n",
        "    \"coz\": \"because\",\n",
        "    \"cuz\": \"because\",\n",
        "    \"cause\": \"because\",\n",
        "    \"cmon\": \"come on\",\n",
        "    \"cmonn\": \"come on\",\n",
        "    \"dafuq\": \"what the fuck\",\n",
        "    \"dafuk\": \"what the fuck\",\n",
        "    \"dis\": \"this\",\n",
        "    \"diss\": \"this\",\n",
        "    \"ma\": \"my\",\n",
        "    \"dono\": \"do not know\",\n",
        "    \"donno\": \"do not know\",\n",
        "    \"dunno\": \"do not know\",\n",
        "    \"fb\": \"facebook\",\n",
        "    \"couldnt\": \"could not\",\n",
        "    \"n\": \"and\",\n",
        "    \"gtg\": \"got to go\",\n",
        "    \"yep\": \"yes\",\n",
        "    \"yw\": \"you are welcome\",\n",
        "    \"im\": \"i am\",\n",
        "    \"youre\": \"you are\",\n",
        "    \"hes\": \"he is\",\n",
        "    \"shes\": \"she is\",\n",
        "    \"theyre\": \"they are\",\n",
        "    \"af\": \"as fuck\",\n",
        "    \"fam\": \"family\",\n",
        "    \"fwd\": \"forward\",\n",
        "    \"ffs\": \"for fuck sake\",\n",
        "    \"fml\": \"fuck my life\",\n",
        "    \"lol\": \"laugh out loud\",\n",
        "    \"lel\": \"laugh out loud\",\n",
        "    \"lool\": \"laugh out loud\",\n",
        "    \"lmao\": \"laugh my ass off\",\n",
        "    \"lmaoo\": \"laugh my ass off\",\n",
        "    \"omg\": \"oh my god\",\n",
        "    \"oomg\": \"oh my god\",\n",
        "    \"omgg\": \"oh my god\",\n",
        "    \"omfg\": \"oh my fucking god\",\n",
        "    \"stfu\": \"shut the fuck up\",\n",
        "    \"awsome\": \"awesome\",\n",
        "    \"imo\": \"in my opinion\",\n",
        "    \"imho\": \"in my humble opinion\",\n",
        "    \"ily\": \"i love you\",\n",
        "    \"ilyy\": \"i love you\",\n",
        "    \"ikr\": \"i know right\",\n",
        "    \"ikrr\": \"i know right\",\n",
        "    \"idk\": \"i do not know\",\n",
        "    \"jk\": \"joking\",\n",
        "    \"lmk\": \"let me know\",\n",
        "    \"nsfw\": \"not safe for work\",\n",
        "    \"hehe\": \"haha\",\n",
        "    \"tmrw\": \"tomorrow\",\n",
        "    \"yt\": \"youtube\",\n",
        "    \"hahaha\": \"haha\",\n",
        "    \"hihi\": \"haha\",\n",
        "    \"pls\": \"please\",\n",
        "    \"ppl\": \"people\",\n",
        "    \"wtf\": \"what the fuck\",\n",
        "    \"wth\": \"what teh hell\",\n",
        "    \"obv\": \"obviously\",\n",
        "    \"nomore\": \"no more\",\n",
        "    \"u\": \"you\",\n",
        "    \"ur\": \"your\",\n",
        "    \"wanna\": \"want to\",\n",
        "    \"luv\": \"love\",\n",
        "    \"imma\": \"i am\",\n",
        "    \"&\": \"and\",\n",
        "    \"thanx\": \"thanks\",\n",
        "    \"til\": \"until\",\n",
        "    \"till\": \"until\",\n",
        "    \"thx\": \"thanks\",\n",
        "    \"pic\": \"picture\",\n",
        "    \"pics\": \"pictures\",\n",
        "    \"gp\": \"doctor\",\n",
        "    \"xmas\": \"christmas\",\n",
        "    \"rlly\": \"really\",\n",
        "    \"boi\": \"boy\",\n",
        "    \"boii\": \"boy\",\n",
        "    \"rly\": \"really\",\n",
        "    \"whch\": \"which\",\n",
        "    \"awee\": \"awsome\",  # or maybe awesome is better\n",
        "    \"sux\": \"sucks\",\n",
        "    \"nd\": \"and\",\n",
        "    \"fav\": \"favourite\",\n",
        "    \"frnds\": \"friends\",\n",
        "    \"info\": \"information\",\n",
        "    \"loml\": \"love of my life\",\n",
        "    \"bffl\": \"best friend for life\",\n",
        "    \"gg\": \"goog game\",\n",
        "    \"xx\": \"love\",\n",
        "    \"xoxo\": \"love\",\n",
        "    \"thats\": \"that is\",\n",
        "    \"homie\": \"best friend\",\n",
        "    \"homies\": \"best friends\"\n",
        "}\n",
        "\n",
        "\n",
        "def word_splitter(token):\n",
        "    _splits = splitter.split(token[1:])\n",
        "    _ninjas = wordninja.split(token[1:])\n",
        "\n",
        "    word_splits = _splits if len(_ninjas) > len(_splits) else _ninjas\n",
        "\n",
        "    if len(token) + 1 / 2 < len(word_splits):\n",
        "        return token\n",
        "\n",
        "    if word_splits:\n",
        "        token = ' '.join(word_splits)\n",
        "    else:\n",
        "        token = token\n",
        "    return token\n",
        "\n",
        "\n",
        "def hashtag_solver(token):\n",
        "    if token.startswith('#') and token[1:].isupper():\n",
        "        return token[1:]\n",
        "\n",
        "    if token.startswith('#'):\n",
        "        # word_splits = re.sub(r\"([A-Z])\", r\" \\1\", word[1:]).split()\n",
        "        myString = re.sub(r'((?<=[a-z])[A-Z]|(?<!\\A)[A-Z](?=[a-z]))', r' \\1', token[1:])\n",
        "        word_splits = myString.split()\n",
        "        # print(\"Splitter {}\".format(word_splits))\n",
        "        if len(word_splits) > 1:\n",
        "            token = ' '.join(word_splits)\n",
        "            # print(\"if {} ### {}\".format(tmp, token))\n",
        "        else:\n",
        "            token = word_splitter(token)\n",
        "            # print(\"Else {} ### {}\".format(tmp, token))\n",
        "\n",
        "    return token\n",
        "\n",
        "def normalize_word(word):\n",
        "    temp = word\n",
        "    while True:\n",
        "        w = re.sub(r\"([a-zA-Z])\\1\\1\", r\"\\1\\1\", temp)\n",
        "        if (w == temp):\n",
        "            break\n",
        "        else:\n",
        "            temp = w\n",
        "    return w\n",
        "\n",
        "\n",
        "def remove_emoji(string):\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                               u\"\\U00002702-\\U000027B0\"\n",
        "                               u\"\\U000024C2-\\U0001F251\"\n",
        "                               \"]+\", flags=re.UNICODE)\n",
        "    return emoji_pattern.sub(r'', string)\n",
        "\n",
        "\n",
        "def refer_normalize(tokens):\n",
        "    words = []\n",
        "    for idx in range(len(tokens)):\n",
        "        if idx + 1 != len(tokens) and tokens[idx].startswith(\"@\") and tokens[idx + 1].startswith(\"@\"):\n",
        "            continue\n",
        "        else:\n",
        "            words.append(tokens[idx])\n",
        "    return words\n",
        "\n",
        "\n",
        "def clean_text(text):\n",
        "    # remove url\n",
        "    text = re.sub(r'http\\S+', '', text)\n",
        "\n",
        "    # fixing apostrope\n",
        "    text = text.replace(\"â€™\", \"'\")\n",
        "\n",
        "    # remove &amp;\n",
        "    text = text.replace('&amp;', 'and ')\n",
        "\n",
        "    # remove \\n\n",
        "    text = re.sub(\"\\\\n\", \"\", text)\n",
        "\n",
        "    # remove leaky elements like ip,user\n",
        "    text = re.sub(\"\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\", \"\", text)\n",
        "\n",
        "    # (')aphostophe  replacement (ie)   you're --> you are\n",
        "    # ( basic dictionary lookup : master dictionary present in a hidden block of code)\n",
        "    # tokenizer = TweetTokenizer()\n",
        "    tokens = text.split()\n",
        "    tokens = refer_normalize(tokens)\n",
        "    tokens = [fill[word] if word in fill else word for word in tokens]\n",
        "    tokens = [fill[word.lower()] if word.lower() in fill else word for word in tokens]\n",
        "    tokens = [slang[word] if word in slang else word for word in tokens]\n",
        "    tokens = [slang[word.lower()] if word.lower() in slang else word for word in tokens]\n",
        "    tokens = [hashtag_solver(word) for word in tokens]\n",
        "    tokens = [normalize_word(word) for word in tokens]\n",
        "    exclude = set(string.punctuation)\n",
        "    text = ' '.join(ch for ch in tokens if ch not in exclude)\n",
        "\n",
        "    # removing usernames\n",
        "    text = re.sub(\"'s\", \"\", text)\n",
        "    text = re.sub(\"'\", \"\", text)\n",
        "    mention_pattern=re.compile(r'@\\w*')\n",
        "    text=re.sub(pattern=mention_pattern, repl='<USER>', string=text)\n",
        "    # emoji remover\n",
        "    text = remove_emoji(text)\n",
        "\n",
        "    text = re.sub(\"&#\\S+\", \"\", text)\n",
        "    text = re.sub(\"RT :\", \"\", text)\n",
        "    text=preprocess(text).lower()\n",
        "    # remove non-ascii\n",
        "    # text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)\n",
        "    # text = re.sub(r'[^\\w\\s]', '', text)\n",
        "\n",
        "    # to lower\n",
        "    return text\n",
        "\n",
        "\n",
        "def dump(data):\n",
        "    def default(obj):\n",
        "        if type(obj).__module__ == np.__name__:\n",
        "            if isinstance(obj, np.ndarray):\n",
        "                return obj.tolist()\n",
        "            else:\n",
        "                return obj.item()\n",
        "        raise TypeError('Unknown type:', type(obj))\n",
        "\n",
        "    json.dumps(data, default=default)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    txt = \"@Jeremy_Hunt @yttr trying to sneak Junior doctors contracts through while media focussed on Chilcott?? You utter cunt. #Fuckyouandyourtoryparty\"\n",
        "    re = clean_text(txt)\n",
        "    print(re)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xjIG52ir0oH_"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import re\n",
        "import json\n",
        "import pickle as pkl\n",
        "import numpy as np\n",
        "import h5py\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from tqdm import tqdm\n",
        "import itertools\n",
        "import random\n",
        "import math\n",
        "import string\n",
        "\n",
        "def load_pkl(path):\n",
        "    data=pkl.load(open(path,'rb'))\n",
        "    return data\n",
        "\n",
        "def read_hdf5(path):\n",
        "    data=h5py.File(path,'rb')\n",
        "    return data\n",
        "\n",
        "def read_csv(path):\n",
        "    data=pd.read_csv(path)\n",
        "    return data\n",
        "\n",
        "def read_csv_sep(path):\n",
        "    data=pd.read_csv(path,sep='\\t')\n",
        "    return data\n",
        "\n",
        "def dump_pkl(path,info):\n",
        "    pkl.dump(info,open(path,'wb'))  \n",
        "\n",
        "def read_json(path):\n",
        "    assert_exits(path)\n",
        "    data=json.load(open(path,'rb'))\n",
        "    '''in anet-qa returns a list'''\n",
        "    return data\n",
        "\n",
        "def pd_pkl(path):\n",
        "    data=pd.read_pickle(path)\n",
        "    return data\n",
        "\n",
        "def read_jsonl(path):\n",
        "    total_info=[]\n",
        "    with open(path,'rb')as f:\n",
        "        d=f.readlines()\n",
        "    for i,info in enumerate(d):\n",
        "        data=json.loads(info)\n",
        "        total_info.append(data)\n",
        "    return total_info\n",
        "\n",
        "class Base_Op(object):\n",
        "    def __init__(self):\n",
        "        self.opt= parse_opt()\n",
        "        self.tasks=self.opt.TASKS.split(',')\n",
        "\n",
        "    def tokenize(self,x):\n",
        "        #x = clean_text(x).split()\n",
        "        #print (x)\n",
        "        x=x.lower().split()\n",
        "        #print (x)\n",
        "        return x\n",
        "\n",
        "    def get_tokens(self,sent):\n",
        "        tokens=self.tokenize(sent)\n",
        "        #print tokens\n",
        "        token_num=[]\n",
        "        for t in tokens:\n",
        "            if t in self.word2idx:\n",
        "                token_num.append(self.word2idx[t])\n",
        "            else:\n",
        "                token_num.append(self.word2idx['UNK'])\n",
        "        if not token_num:\n",
        "            token_num.append(self.word2idx['UNK'])\n",
        "        return token_num\n",
        "\n",
        "    def get_words(self, sent):\n",
        "        tokens = self.tokenize(sent)\n",
        "        # print tokens\n",
        "        words = []\n",
        "        for t in tokens:\n",
        "            if t in self.word2idx:\n",
        "                words.append(t)\n",
        "            else:\n",
        "                words.append('UNK')\n",
        "        return words\n",
        "\n",
        "    def token_sent(self):\n",
        "        cur=0\n",
        "        datasets=[]\n",
        "        for dataset in self.tasks:\n",
        "            name=os.path.join(self.opt.SPLIT_DATASET,dataset)+'.pkl'\n",
        "            data=pkl.load(open(name,'rb'))\n",
        "            datasets.append(data)\n",
        "        print ('Total number of datasets:',len(self.tasks))\n",
        "        \n",
        "       \n",
        "        for i in range(6):\n",
        "            cur_total=[]\n",
        "            for data in datasets:\n",
        "                try:\n",
        "                    cur_total.extend(data[str(i)])\n",
        "                except Exception as e:\n",
        "                    pass\n",
        "\n",
        "            for info in cur_total:\n",
        "                tweet=info['sent']\n",
        "                tokens=self.tokenize(tweet)\n",
        "                for t in tokens:\n",
        "                    if t not in self.word_count:\n",
        "                        self.word_count[t]=1\n",
        "                    else:\n",
        "                        self.word_count[t]+=1\n",
        "            print ('Length of current sentences:',len(cur_total))\n",
        "        for word in self.word_count.keys():\n",
        "            if self.word_count[word]>=self.opt.MIN_OCC:\n",
        "                self.word2idx[word]=cur\n",
        "                self.idx2word.append(word)\n",
        "                cur+=1\n",
        "\n",
        "        if 'PAD' not in self.word2idx:\n",
        "            self.idx2word.append('UNK')\n",
        "            self.word2idx['UNK']=0\n",
        "\n",
        "        if 'UNK' not in self.word2idx:\n",
        "            self.idx2word.append('UNK')\n",
        "            self.word2idx['UNK']=len(self.idx2word)-1   \n",
        "\n",
        "        dump_pkl(os.path.join(self.tasks[0],'dictionary.pkl'),[self.word2idx,self.idx2word])\n",
        "\n",
        "    def create_dict(self):\n",
        "        self.word_count={}\n",
        "        self.word2idx={}\n",
        "        self.idx2word=[]\n",
        "        self.token_sent()\n",
        "\n",
        "    def create_embedding(self):\n",
        "\n",
        "        word2emb={}\n",
        "        with open(self.opt.GLOVE_PATH,'r') as f:\n",
        "            entries=f.readlines()\n",
        "        emb_dim=len(entries[0].split(' '))-1\n",
        "        weights=np.zeros((len(self.idx2word),emb_dim),dtype=np.float32)\n",
        "        for entry in entries:\n",
        "            word=entry.split(' ')[0]\n",
        "            word2emb[word]=np.array(list(map(float,entry.split(' ')[1:])))\n",
        "        for idx,word in enumerate(self.idx2word):\n",
        "            if word not in word2emb:\n",
        "                continue\n",
        "            weights[idx]=word2emb[word]\n",
        "\n",
        "        np.save(os.path.join(self.tasks[0],'glove_embedding.npy'),weights)\n",
        "        return weights\n",
        "\n",
        "    def init_dict(self):\n",
        "        if self.opt.CREATE_DICT:\n",
        "            print ('Creating Dictionary...')\n",
        "            self.create_dict()\n",
        "        else:\n",
        "            created_dict=load_pkl(os.path.join(self.tasks[0],'dictionary.pkl'))\n",
        "\n",
        "\n",
        "            self.word2idx=created_dict[0]\n",
        "            self.idx2word=created_dict[1]\n",
        "\n",
        "        if self.opt.CREATE_EMB:\n",
        "            print ('Creating Embedding...;')\n",
        "            self.create_embedding()\n",
        "\n",
        "        self.ntoken()\n",
        "\n",
        "    def ntoken(self):\n",
        "        self.ntokens=len(self.word2idx)\n",
        "        print ('Number of Tokens:',self.ntokens)\n",
        "        return self.ntokens\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.word2idx)\n",
        "\n",
        "class Wraped_Data(Base_Op):\n",
        "    def __init__(self,opt,dictionary,split_data,test_num,mode='training',source='dt'):\n",
        "        #hate for hate speech detection dataset and lingo for hatelingo\n",
        "        super(Wraped_Data,self).__init__()\n",
        "        self.opt= parse_opt()\n",
        "        random.seed(opt.SEED)\n",
        "        self.dictionary=dictionary\n",
        "        self.split_data=split_data\n",
        "        self.test_num=test_num\n",
        "        self.mode=mode\n",
        "        self.source=source\n",
        "        #self.batch_size_dict = {\"dt\": 64, \"wz\": 50, \"hatelingo\": 28, \"founta\": 260}\n",
        "        self.class_size_dict ={\n",
        "                'wz':3,'dt':3,'founta':4,'hatelingo':5,\n",
        "                'offenseval_c':3,'semeval_a':11\n",
        "                }\n",
        "        #self.batch_size = self.batch_size_dict.get(source, opt.BATCH_SIZE)\n",
        "        self.batch_size=opt.BATCH_SIZE\n",
        "        self.classes = self.class_size_dict.get(source, 2)\n",
        "        #loading the data: later used for batch iteration\n",
        "        self.entries=self.load_tr_val_entries()\n",
        "        self.num_iters=int(math.ceil(len(self.entries) * 1.0 / self.batch_size))\n",
        "        self.last_batch=len(self.entries) % self.batch_size\n",
        "        #print (type(self.num_iters),type(self.last_batch))\n",
        "        self.cur_iter=0\n",
        "        print(mode)\n",
        "        print(\"Task name {} Batch size {} Class size {}\".format(self.source, self.batch_size, self.classes))\n",
        "        print('The length of all entries is:',len(self.entries))\n",
        "        print ('Information about batch loader: number of iteration:',self.num_iters,'number of last batch:',self.last_batch)\n",
        "\n",
        "        self.length=opt.LENGTH\n",
        "\n",
        "    def load_tr_val_entries(self):\n",
        "        all_data=[]\n",
        "        if self.mode=='training':\n",
        "            for i in range(self.opt.CROSS_VAL):\n",
        "                if i==self.test_num:\n",
        "                    continue\n",
        "                all_data.extend(self.split_data[str(i)])\n",
        "        else:\n",
        "            all_data.extend(self.split_data[str(self.test_num)])\n",
        "        entries=[]\n",
        "        for i,info in enumerate(all_data):\n",
        "            sent=info['sent']\n",
        "            label=info['label']\n",
        "            bert_token=info['bert_token']\n",
        "            entry={\n",
        "                    'sent':sent,\n",
        "                    'answer':label,\n",
        "                    'bert_token':bert_token\n",
        "                    }\n",
        "            entries.append(entry)\n",
        "        #shuffle the dataset\n",
        "        random.shuffle(entries)\n",
        "        return entries\n",
        "\n",
        "    def padding_bert(self,tokens,length):\n",
        "        if len(tokens)<length:\n",
        "            padding=[0]*(length-len(tokens))\n",
        "            tokens=tokens+padding\n",
        "        else:\n",
        "            tokens=tokens[:length]\n",
        "        return tokens\n",
        "\n",
        "    def padding_sent(self,tokens,length):\n",
        "        if len(tokens)<length:\n",
        "            padding=[0]*(length-len(tokens))\n",
        "            tokens=tokens+padding\n",
        "        else:\n",
        "            tokens=tokens[:length]\n",
        "        return tokens\n",
        "\n",
        "    def get_masks(self,tokens,length):\n",
        "        masks = [1]*(len(tokens)) + [0]*(length-len(tokens))\n",
        "        masks = masks[:length]\n",
        "        return masks\n",
        "\n",
        "\n",
        "    def next_batch(self):\n",
        "        batch_info={}\n",
        "        #if self.source in [\"wz\",\"dt\",\"founta\"] and self.mode == \"test\":\n",
        "            #print(\"Loading iter {} # {}\".format(self.mode, self.cur_iter))\n",
        "        if self.cur_iter==self.num_iters-1 :\n",
        "            if self.last_batch>0:\n",
        "                cur_entry=self.entries[self.cur_iter*self.batch_size:self.last_batch+self.cur_iter*self.batch_size]\n",
        "                if self.mode=='training':\n",
        "                    self.cur_iter=-1\n",
        "                    #tmp_entry = self.entries[self.cur_iter*self.batch_size:(1+self.cur_iter)*abs(len(cur_entry)-self.batch_size)]\n",
        "                    #cur_entry.extend(tmp_entry)\n",
        "                    random.shuffle(self.entries)\n",
        "                elif self.mode=='test' or self.mode==\"val\":\n",
        "                    self.cur_iter=-1\n",
        "                    #tmp_entry = self.entries[self.cur_iter*self.batch_size:(1+self.cur_iter)*abs(len(cur_entry)-self.batch_size)]\n",
        "                    #cur_entry.extend(tmp_entry)\n",
        "            elif self.last_batch==0:\n",
        "                cur_entry=self.entries[self.cur_iter*self.batch_size:(1+self.cur_iter)*self.batch_size]\n",
        "                if self.mode=='training': \n",
        "                    self.cur_iter=-1\n",
        "                    random.shuffle(self.entries)\n",
        "                elif self.mode=='test' or self.mode==\"val\":\n",
        "                    self.cur_iter=-1\n",
        "            \"\"\"\n",
        "            if the mode is training, then restart from the beginning\n",
        "            cur_iters=0\n",
        "            and shuffle the dataset again\n",
        "            \"\"\"\n",
        "        else:\n",
        "            cur_entry=self.entries[self.cur_iter*self.batch_size:(1+self.cur_iter)*self.batch_size]\n",
        "        if len(cur_entry)>0:\n",
        "            cur_entry = sorted(cur_entry, key=lambda k:len(k['sent'].split()), reverse=True)\n",
        "            self.length = len(cur_entry[0]['sent'].split())\n",
        "            batch_tokens=np.zeros([len(cur_entry),self.length],dtype=np.int64)\n",
        "            batch_label=np.zeros([len(cur_entry),self.classes],dtype=np.int64)\n",
        "            batch_bert=np.zeros([len(cur_entry),64],dtype=np.int64)\n",
        "            batch_masks=np.zeros([len(cur_entry),64],dtype=np.int64)\n",
        "            batch_att_masks = np.zeros([len(cur_entry), self.length], dtype=np.int64)\n",
        "            batch_words=[]\n",
        "            for k in range(len(cur_entry)):\n",
        "                chunck=cur_entry[k]\n",
        "                #for debuging print key of data, make sure load all data\n",
        "                #print (chunck['key'],self.classes)\n",
        "                #get batch tokens\n",
        "                sent=chunck['sent']\n",
        "                bert_tokens=chunck['bert_token']\n",
        "                tokens=self.dictionary.get_tokens(sent)\n",
        "                pad_tokens=self.padding_sent(tokens,self.length)\n",
        "                bert_pad=self.padding_bert(bert_tokens,64)\n",
        "                #print (tokens,bert_tokens)\n",
        "                masks=mask=[int(num>0) for num in bert_pad]\n",
        "                batch_tokens[k,:]=np.array((pad_tokens),dtype=np.int64)\n",
        "                batch_bert[k,:]=np.array((bert_pad),dtype=np.int64)\n",
        "                batch_masks[k,:]=np.array((mask),dtype=np.int64)\n",
        "                att_masks = self.get_masks(tokens, self.length)\n",
        "                batch_att_masks[k,:] = np.array((att_masks),dtype=np.int64)\n",
        "\n",
        "                words=self.dictionary.get_words(sent)\n",
        "                words = words + ['PAD']*(self.length-len(words))\n",
        "                words = words[:self.length]\n",
        "                batch_words.append(words)\n",
        "\n",
        "                #get batch labels\n",
        "                label=chunck['answer']\n",
        "                if self.source == 'semeval_a':\n",
        "                    target=np.array(label)\n",
        "                else:\n",
        "                    target=np.zeros((self.classes),dtype=np.float32)\n",
        "                    target[label]=1.0\n",
        "                batch_label[k,:]=target\n",
        "            batch_tokens=torch.from_numpy(batch_tokens)\n",
        "            batch_bert=torch.from_numpy(batch_bert)\n",
        "            batch_label=torch.from_numpy(batch_label)\n",
        "            batch_masks=torch.from_numpy(batch_masks)\n",
        "            batch_att_masks=torch.from_numpy(batch_att_masks)\n",
        "            batch_info['tokens']=batch_tokens\n",
        "            batch_info['label']=batch_label\n",
        "            batch_info['bert_tokens']=batch_bert\n",
        "            batch_info['words']=batch_words\n",
        "            batch_info['masks']=batch_masks\n",
        "            batch_info['att_masks'] = batch_att_masks\n",
        "            self.cur_iter+=1\n",
        "            return batch_info\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_iters\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "II8S7ta43jqK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b50e898c-6a38-40fc-f269-875ae3a6e849"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.21.0-py3-none-any.whl (4.7 MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4.7 MB 4.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.8.1-py3-none-any.whl (101 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 101 kB 11.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.12.0)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 596 kB 56.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6.6 MB 37.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Installing collected packages: pyyaml, tokenizers, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.8.1 pyyaml-6.0 tokenizers-0.12.1 transformers-4.21.0\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j5h-9NoU1qz8"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn.utils.rnn import pad_packed_sequence\n",
        "\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "import copy \n",
        "from transformers import BertForSequenceClassification,BertConfig\n",
        "\n",
        "def clones(module,N):\n",
        "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n",
        "\n",
        "class LSTM_Update(nn.Module):\n",
        "    def __init__(self,in_dim,hidden_dim,dropout):\n",
        "        super(LSTM_Update,self).__init__()\n",
        "        self.in_dim=in_dim\n",
        "        self.hidden_dim=hidden_dim\n",
        "        self.dropout=nn.Dropout(dropout)\n",
        "        \n",
        "        self.in_proj=clones(nn.Linear(in_dim,hidden_dim),4)\n",
        "        self.hidden_proj=clones(nn.Linear(hidden_dim,hidden_dim),4)\n",
        "        self.context_proj=clones(nn.Linear(hidden_dim,hidden_dim),3)\n",
        "        \n",
        "    def forward(self,word,hidden,context,c_hate):\n",
        "        total=[] #stores i_t,f_t,o_t\n",
        "        for i in range(3):\n",
        "            result=torch.sigmoid(self.in_proj[i](word)+self.hidden_proj[i](hidden)+self.context_proj[i](context))\n",
        "            total.append(result)\n",
        "        c_t=total[1] * context + total[0] * c_hate\n",
        "        h_t=total[2] * torch.tanh(c_t)\n",
        "        return h_t,c_t\n",
        "    \n",
        "class Bi_LSTM_Update(nn.Module):\n",
        "    def __init__(self,in_dim,hidden_dim):\n",
        "        super(Bi_LSTM_Update,self).__init__()\n",
        "        self.in_dim=in_dim\n",
        "        self.hidden_dim=hidden_dim\n",
        "        \n",
        "        self.in_proj=clones(nn.Linear(in_dim,hidden_dim),4)\n",
        "        self.hidden_proj=clones(nn.Linear(hidden_dim,hidden_dim),4)\n",
        "        self.context_proj=clones(nn.Linear(hidden_dim,hidden_dim),3)\n",
        "        \n",
        "    def forward(self,word,hidden,context):\n",
        "        total=[] #stores i_t,f_t,o_t\n",
        "        for i in range(3):\n",
        "            result=torch.sigmoid(self.in_proj[i](word)+self.hidden_proj[i](hidden)+self.context_proj[i](context))\n",
        "            total.append(result)\n",
        "        c_hate=torch.tanh(self.in_proj[3](word) + self.hidden_proj[3](hidden))\n",
        "        c_t=total[1] * context + total[0] * c_hate\n",
        "        h_t=total[2] * torch.tanh(c_t)\n",
        "        return h_t,c_t\n",
        "    \n",
        "class Bi_LSTM(nn.Module):\n",
        "    def __init__(self,in_dim,hidden_dim,dropout):\n",
        "        super(Bi_LSTM,self).__init__()\n",
        "        self.lstms=clones(Bi_LSTM_Update(in_dim,hidden_dim),2)\n",
        "        self.hidden_dim=hidden_dim\n",
        "        \n",
        "    def forward(self,seq):\n",
        "        h0=[Variable(torch.zeros(seq.size()[0],self.hidden_dim).cuda()),Variable(torch.zeros(seq.size()[0],self.hidden_dim).cuda())]\n",
        "        c0=[Variable(torch.zeros(seq.size()[0],self.hidden_dim).cuda()),Variable(torch.zeros(seq.size()[0],self.hidden_dim).cuda())]\n",
        "        hidden=[[h0[0].unsqueeze(1)],[h0[1].unsqueeze(1)]]\n",
        "        context=[[c0[0]],[c0[1]]]\n",
        "        for idx in range(seq.size()[1]):\n",
        "            word=[seq[:,idx,:],seq[:,-(idx+1),:]]\n",
        "            for i, w in enumerate(word):\n",
        "                h=hidden[i][0].squeeze()\n",
        "                c=context[i][0]\n",
        "                new_h,new_c=self.lstms[i](word[i],h,c)\n",
        "                if i==0:\n",
        "                    hidden[i].append(new_h.unsqueeze(1))\n",
        "                    context[i].append(new_c)\n",
        "                else:\n",
        "                    hidden[i].insert(0,new_h.unsqueeze(1))\n",
        "                    context[i].insert(0,new_c)\n",
        "        forward_h=torch.cat(hidden[0],dim=1)\n",
        "        backward_h=torch.cat(hidden[1],dim=1)\n",
        "        final_h=torch.cat((forward_h,backward_h),dim=2)\n",
        "        return final_h\n",
        "\n",
        "'''class Coupled_Layer(nn.Module):\n",
        "    def __init__(self,in_dim,hidden_dim,dropout=0.3):\n",
        "        super(Coupled_Layer,self).__init__()\n",
        "        \"\"\"\n",
        "        two LSTMs sharing information with each other\n",
        "        \"\"\"\n",
        "        self.lstm=clones(LSTM_Update(in_dim,hidden_dim,dropout),2)\n",
        "        \n",
        "        self.proj_word=clones(nn.Linear(in_dim,hidden_dim),2)\n",
        "        \n",
        "        self.gate_proj_w=clones(nn.Linear(in_dim,1),2)#W_g\n",
        "        self.gate_proj_u=clones(nn.Linear(hidden_dim,1),2)#U_g\n",
        "        \n",
        "        self.proj_hidden=clones(nn.Linear(hidden_dim,hidden_dim),2)\n",
        "        \n",
        "        self.hidden_dim=hidden_dim\n",
        "        \n",
        "    def forward(self,seq,num_task=0):\n",
        "        h0=[Variable(torch.zeros(seq.size()[0],self.hidden_dim).cuda()),Variable(torch.zeros(seq.size()[0],self.hidden_dim).cuda())]\n",
        "        c0=[Variable(torch.zeros(seq.size()[0],self.hidden_dim).cuda()),Variable(torch.zeros(seq.size()[0],self.hidden_dim).cuda())]\n",
        "        \n",
        "        other=1-num_task\n",
        "        for idx in range(seq.size()[1]):\n",
        "            word=seq[:,idx,:]\n",
        "            proj_word=self.proj_word[num_task](word)\n",
        "            g_own=torch.sigmoid(self.gate_proj_w[num_task](word) + self.gate_proj_u[num_task](h0[num_task]))\n",
        "            g_other=torch.sigmoid(self.gate_proj_w[num_task](word) + self.gate_proj_u[other](h0[other]))\n",
        "            c_hate=torch.tanh(proj_word + g_own*self.proj_hidden[num_task](h0[num_task]) + g_other*self.proj_hidden[other](h0[other]))\n",
        "            h,c=self.lstm[num_task](word,h0[num_task],c0[num_task],c_hate)\n",
        "            #print(h.shape,c.shape)\n",
        "            h0[num_task]=h\n",
        "            c0[num_task]=c\n",
        "            \n",
        "            \"\"\"\n",
        "           updating the hidden states and context in the other LSTM \n",
        "            \"\"\"\n",
        "            proj_word=self.proj_word[other](word)\n",
        "            c_hate=torch.tanh(proj_word + g_own + g_other)\n",
        "            h,c=self.lstm[other](word,h0[other],c0[other],c_hate)\n",
        "            h0[other]=h\n",
        "            c0[other]=c\n",
        "            \n",
        "        return h0[num_task],c0[num_task]'''\n",
        "\n",
        "#extension for the coupling layer\n",
        "#previously for binary tasks now extend to multiple tasks\n",
        "class Coupled_Layer(nn.Module):\n",
        "    def __init__(self,in_dim,hidden_dim,num_task,dropout=0.3):\n",
        "        super(Coupled_Layer,self).__init__()\n",
        "        \"\"\"\n",
        "        two LSTMs sharing information with each other\n",
        "        \"\"\"\n",
        "        self.lstm=clones(LSTM_Update(in_dim,hidden_dim,dropout),num_task)\n",
        "        \n",
        "        self.proj_word=clones(nn.Linear(in_dim,hidden_dim),num_task)#W_c\n",
        "        \n",
        "        self.gate_proj_w=clones(nn.Linear(in_dim,1),num_task)#W_gc\n",
        "        self.gate_proj_u=clones(nn.Linear(hidden_dim,1),num_task)#U_gc\n",
        "        \n",
        "        self.proj_hidden=clones(nn.Linear(hidden_dim,hidden_dim),num_task*num_task)#U_c\n",
        "        \n",
        "        self.hidden_dim=hidden_dim\n",
        "        self.tasks=num_task\n",
        "        \n",
        "    def forward(self,seq,num_task=0):\n",
        "        h0=[Variable(torch.zeros(seq.size()[0],self.hidden_dim).cuda())] * self.tasks\n",
        "        c0=[Variable(torch.zeros(seq.size()[0],self.hidden_dim).cuda())] * self.tasks\n",
        "        \n",
        "        h_latest=[Variable(torch.zeros(seq.size()[0],self.hidden_dim).cuda())] * self.tasks\n",
        "        c_latest=[Variable(torch.zeros(seq.size()[0],self.hidden_dim).cuda())] * self.tasks\n",
        "        for idx in range(seq.size()[1]):\n",
        "            word=seq[:,idx,:]\n",
        "            #proj_word=self.proj_word[num_task](word)\n",
        "            #print(h.shape,c.shape)  \n",
        "            for k in range(self.tasks):\n",
        "                total=self.proj_word[k](word)\n",
        "                for j in range(self.tasks):\n",
        "                    g_own=torch.sigmoid(self.gate_proj_w[k](word) + self.gate_proj_u[j](h0[j]))#g_jk\n",
        "                    cur_other=g_own * self.proj_hidden[k*self.tasks+j](h0[j])\n",
        "                    total=total+cur_other\n",
        "                #update hidden states in other lstms\n",
        "                c_hate=torch.tanh(total)\n",
        "                h,c=self.lstm[num_task](word,h0[k],c0[k],c_hate) \n",
        "                h_latest[k]=h\n",
        "                c_latest[k]=c \n",
        "            h0=h_latest\n",
        "            c0=c_latest\n",
        "        return h0[num_task],c0[num_task]\n",
        "\n",
        "class Local_Layer(nn.Module):\n",
        "    def __init__(self,in_dim,hidden_dim,num_task,dropout=0.3):\n",
        "        super(Coupled_Layer,self).__init__()\n",
        "        \"\"\"\n",
        "        two LSTMs sharing information with each other\n",
        "        \"\"\"\n",
        "        self.lstm=clones(LSTM_Update(in_dim,hidden_dim,dropout),num_task)\n",
        "        \n",
        "        self.proj_first=clones(nn.Linear(in_dim,hidden_dim),num_task)\n",
        "        self.proj_word=clones(nn.Linear(in_dim,hidden_dim),num_task)#W_c\n",
        "        \n",
        "        self.gate_proj_w=clones(nn.Linear(in_dim,1),num_task)#W_gc\n",
        "        self.gate_proj_u=clones(nn.Linear(hidden_dim,1),num_task)#U_gc\n",
        "        \n",
        "        self.proj_hidden=clones(nn.Linear(hidden_dim,hidden_dim),num_task*num_task)#U_cc\n",
        "        self.proj_hidden=clones(nn.Linear(hidden_dim,hidden_dim),num_task*num_task)#U_c\n",
        "        self.proj_hidden=clones(nn.Linear(hidden_dim,hidden_dim),num_task)#U_gf\n",
        "        self.hidden_dim=hidden_dim\n",
        "        self.tasks=num_task\n",
        "        \n",
        "    def forward(self,seq,num_task=0):\n",
        "        h0=[Variable(torch.zeros(seq.size()[0],self.hidden_dim).cuda())] * self.tasks\n",
        "        c0=[Variable(torch.zeros(seq.size()[0],self.hidden_dim).cuda())] * self.tasks\n",
        "        \n",
        "        h_latest=[Variable(torch.zeros(seq.size()[0],self.hidden_dim).cuda())] * self.tasks\n",
        "        c_latest=[Variable(torch.zeros(seq.size()[0],self.hidden_dim).cuda())] * self.tasks\n",
        "        for idx in range(seq.size()[1]):\n",
        "            word=seq[:,idx,:]\n",
        "            #proj_word=self.proj_word[num_task](word)\n",
        "            #print(h.shape,c.shape)  \n",
        "            for k in range(self.tasks):\n",
        "                total=self.proj_word[k](word)\n",
        "                for j in range(self.tasks):\n",
        "                    g_own=torch.sigmoid(self.gate_proj_w[k](word) + self.gate_proj_u[j](h0[j]))#g_jk\n",
        "                    cur_other=g_own * self.proj_hidden[k*self.tasks+j](h0[j])\n",
        "                    total=total+cur_other\n",
        "                #update hidden states in other lstms\n",
        "                c_t=torch.tanh(total)\n",
        "                proj_x=self.proj_first[k](word)\n",
        "                #computation of LF\n",
        "                c_hate=torch.tanh(proj_x+c_t+LF)\n",
        "                h,c=self.lstm[num_task](word,h0[k],c0[k],c_hate) \n",
        "                h_latest[k]=h\n",
        "                c_latest[k]=c \n",
        "            h0=h_latest\n",
        "            c0=c_latest\n",
        "        return h0[num_task],c0[num_task]    \n",
        "    \n",
        "class Shared_Layer(nn.Module):\n",
        "    def __init__(self,in_dim,hidden_dim,num_task,gate,dropout=0.3):\n",
        "        super(Shared_Layer,self).__init__()\n",
        "        \"\"\"\n",
        "        two LSTMs sharing information with each other\n",
        "        \"\"\"\n",
        "        self.tasks=num_task\n",
        "        self.gate=gate\n",
        "        \n",
        "        self.lstm=clones(LSTM_Update(in_dim,hidden_dim,dropout),num_task)\n",
        "        self.shared_lstm=Bi_LSTM(in_dim,hidden_dim,dropout)\n",
        "        # self.shared_bert=BertForSequenceClassification.from_pretrained(\n",
        "        #     'bert-base-uncased',\n",
        "        #     num_labels=num_task,\n",
        "        #     output_attentions=False,\n",
        "        #     output_hidden_states=True\n",
        "        # )\n",
        "        self.proj_word=clones(nn.Linear(in_dim,hidden_dim),num_task)\n",
        "        self.gate_proj_w=clones(nn.Linear(in_dim,1),num_task)#W_g\n",
        "        self.gate_proj_u=clones(nn.Linear(hidden_dim,1),num_task)#U_g\n",
        "        self.proj_hidden=clones(nn.Linear(hidden_dim,hidden_dim),num_task)\n",
        "        \n",
        "        self.shared_hidden_proj=nn.Linear(2*hidden_dim,hidden_dim)\n",
        "        self.shared_gate_w=clones(nn.Linear(in_dim,1),num_task)#W_g\n",
        "        self.shared_gate_u=clones(nn.Linear(2*hidden_dim,1),num_task)#U_g\n",
        "        \n",
        "        self.bert_proj=nn.Linear(768,2*hidden_dim)\n",
        "        self.bert_proj1=nn.Linear(768,hidden_dim)\n",
        "        self.hidden_dim=hidden_dim\n",
        "        \n",
        "    def forward(self,seq,num_task=0):\n",
        "        h0=[Variable(torch.zeros(seq.size()[0],self.hidden_dim).cuda())] * self.tasks\n",
        "        c0=[Variable(torch.zeros(seq.size()[0],self.hidden_dim).cuda())] * self.tasks\n",
        "        \n",
        "        shared_hidden=self.shared_lstm(seq)\n",
        "        '''shared=self.shared_lstm(bert_tokens,token_type_ids=None,attention_mask=masks)\n",
        "        shared_hidden=self.bert_proj(shared[1][-1])\n",
        "        shared=self.shared_bert(bert_tokens,token_type_ids=None,attention_mask=masks)\n",
        "        global_bert=self.bert_proj1(shared[1][-1][:,0,:])'''\n",
        "        #print (global_bert.shape)\n",
        "        for idx in range(seq.size()[1]):\n",
        "            word=seq[:,idx,:]\n",
        "            hidden=shared_hidden[:,idx,:]\n",
        "            proj_word=self.proj_word[num_task](word)\n",
        "            g_own=torch.sigmoid(self.gate_proj_w[num_task](word) + self.gate_proj_u[num_task](h0[num_task]))\n",
        "            g_share=torch.sigmoid(self.shared_gate_w[num_task](word) + self.shared_gate_u[num_task](hidden))\n",
        "            c_hate=torch.tanh(proj_word + g_own*self.proj_hidden[num_task](h0[num_task]) + g_share*self.shared_hidden_proj(hidden))\n",
        "            h,c=self.lstm[num_task](word,h0[num_task],c0[num_task],c_hate) \n",
        "            #print(h.shape,c.shape)\n",
        "            h0[num_task]=h\n",
        "            c0[num_task]=c  \n",
        "        #global_fusion=self.gate(h0[num_task],global_bert)\n",
        "        global_fusion=h0[num_task]\n",
        "        #print (global_fusion.shape)\n",
        "        return global_fusion,c0[num_task]\n",
        "\n",
        "    \n",
        "class Full_RNN(nn.Module):\n",
        "    def __init__(self,in_dim,num_hidden,num_layer,bidirect,dropout,rnn_type='LSTM'):\n",
        "        super(Full_RNN,self).__init__()\n",
        "        rnn_cls=nn.LSTM if rnn_type=='LSTM' else nn.GRU\n",
        "        self.rnn=rnn_cls(in_dim,num_hidden,num_layer,bidirectional=bidirect,dropout=dropout,batch_first=True)\n",
        "        self.in_dim=in_dim\n",
        "        self.num_hidden=num_hidden\n",
        "        self.num_layer=num_layer\n",
        "        self.rnn_type=rnn_type\n",
        "        self.num_bidirect=1+int(bidirect)\n",
        "        self.bidirect=bidirect\n",
        "        \n",
        "    def init_hidden(self,batch):\n",
        "        weight=next(self.parameters()).data\n",
        "        hid_shape=(self.num_layer * self.num_bidirect,batch,self.num_hidden)\n",
        "        if self.rnn_type =='LSTM':\n",
        "            return (Variable(weight.new(*hid_shape).zero_().cuda()),\n",
        "                    Variable(weight.new(*hid_shape).zero_().cuda()))\n",
        "        else:\n",
        "            return Variable(weight.new(*hid_shape).zero_()).cuda()\n",
        "    \n",
        "    def forward(self,x):\n",
        "        batch=x.size(0)\n",
        "        hidden=self.init_hidden(batch)\n",
        "        self.rnn.flatten_parameters()\n",
        "        output,hidden=self.rnn(x,hidden)\n",
        "        if self.bidirect:\n",
        "            hidden = torch.cat((hidden[0][0], hidden[0][1]), dim=1)\n",
        "            hidden = hidden.squeeze()\n",
        "        else:\n",
        "            hidden = hidden[0].squeeze()\n",
        "        return output,hidden\n",
        "\n",
        "class BiLSTMPacked(nn.Module):\n",
        "    def __init__(self, in_dim, num_hidden, num_layer, bidirect, dropout, rnn_type='LSTM'):\n",
        "        super(BiLSTMPacked, self).__init__()\n",
        "        rnn_cls = nn.LSTM if rnn_type == 'LSTM' else nn.GRU\n",
        "        self.rnn = rnn_cls(in_dim, num_hidden, num_layer, bidirectional=bidirect, dropout=dropout, batch_first=True)\n",
        "        self.num_layer=num_layer\n",
        "        self.num_hidden=num_hidden\n",
        "        self.rnn_type=rnn_type\n",
        "        self.num_bidirect=1+int(bidirect)\n",
        "        self.bidirect=bidirect\n",
        "\n",
        "    def init_hidden(self,batch):\n",
        "        weight=next(self.parameters()).data\n",
        "        hid_shape=(self.num_layer * self.num_bidirect,batch,self.num_hidden)\n",
        "        if self.rnn_type =='LSTM':\n",
        "            return (Variable(weight.new(*hid_shape).zero_().cuda()),\n",
        "                    Variable(weight.new(*hid_shape).zero_().cuda()))\n",
        "        else:\n",
        "            return Variable(weight.new(*hid_shape).zero_()).cuda()\n",
        "\n",
        "    def forward(self, x):\n",
        "        hidden = self.init_hidden(64)\n",
        "        self.rnn.flatten_parameters()\n",
        "        pack_output, hidden = self.rnn(x)\n",
        "        #print(pack_output.shape, hidden.shape)\n",
        "        output, _ = pad_packed_sequence(pack_output, batch_first=True)\n",
        "        batch = output.size(0)\n",
        "        if self.bidirect:\n",
        "            hidden = hidden[0].view(self.num_layer,self.num_bidirect,batch,self.num_hidden)[-1]\n",
        "            hidden = torch.cat((hidden[0], hidden[1]), dim=1)\n",
        "        else:\n",
        "            hidden = hidden[0].view(self.num_layer,self.num_bidirect,batch,self.num_hidden)[-1]\n",
        "            hidden = hidden.squeeze()\n",
        "        return output, hidden\n",
        "\n",
        "class CNN_Model(nn.Module):\n",
        "    def __init__(self,in_dim,filter_size,num_filter):\n",
        "        super(CNN_Model,self).__init__()\n",
        "        self.in_dim=in_dim\n",
        "        filter_sizes=[int(fsz) for fsz in filter_size.split(',')]\n",
        "        self.conv=nn.ModuleList([nn.Conv2d(1,num_filter,(fsz,in_dim)) for fsz in filter_sizes])\n",
        "        self.pool=nn.MaxPool1d(kernel_size=4, stride=4)\n",
        "        \n",
        "    def forward(self,emb):\n",
        "        emb=emb.unsqueeze(1)#B,1,L,D\n",
        "        conv_result=[F.relu(conv(emb)) for conv in self.conv]\n",
        "        pool_result=[F.max_pool2d(input=x_i,kernel_size=(x_i.shape[2],x_i.shape[3])) for x_i in conv_result]\n",
        "        mid=[torch.squeeze(x_i) for x_i in pool_result]\n",
        "        final=torch.cat(mid,1)\n",
        "        return final\n",
        "    \n",
        "class Part_RNN(nn.Module):\n",
        "    def __init__(self,in_dim,num_hidden,num_layer,bidirect,dropout,rnn_type='LSTM'):\n",
        "        super(Part_RNN,self).__init__()\n",
        "        rnn_cls=nn.LSTM if rnn_type=='LSTM' else nn.GRU\n",
        "        self.rnn=rnn_cls(in_dim,num_hidden,num_layer,bidirectional=bidirect,dropout=dropout,batch_first=True)\n",
        "        self.in_dim=in_dim\n",
        "        self.num_hidden=num_hidden\n",
        "        self.num_layer=num_layer\n",
        "        self.rnn_type=rnn_type\n",
        "        self.num_bidirect=1+int(bidirect)\n",
        "        \n",
        "    def init_hidden(self,batch):\n",
        "        weight=next(self.parameters()).data\n",
        "        hid_shape=(self.num_layer * self.num_bidirect,batch,self.num_hidden)\n",
        "        if self.rnn_type =='LSTM':\n",
        "            return (Variable(weight.new(*hid_shape).zero_().cuda()),\n",
        "                    Variable(weight.new(*hid_shape).zero_().cuda()))\n",
        "        else:\n",
        "            return Variable(weight.new(*hid_shape).zero_()).cuda()\n",
        "    \n",
        "    def forward(self,x):\n",
        "        batch=x.size(0)\n",
        "        hidden=self.init_hidden(batch)\n",
        "        self.rnn.flatten_parameters()\n",
        "        output,hidden=self.rnn(x,hidden)\n",
        "        return output[:,-1,:]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s6x8WgZ617sV"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import os\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "import math\n",
        "\n",
        "opt= parse_opt()\n",
        "class Word_Embedding(nn.Module):\n",
        "    def __init__(self,ntoken,emb_dim,dropout):\n",
        "        super(Word_Embedding,self).__init__()\n",
        "        self.emb=nn.Embedding(ntoken+1,emb_dim,padding_idx=ntoken)\n",
        "        self.dropout=nn.Dropout(dropout)\n",
        "        self.ntoken=ntoken\n",
        "        self.emb_dim=emb_dim\n",
        "        self.tasks=opt.TASKS.split(',')\n",
        "\n",
        "    def init_embedding(self):\n",
        "        print ('Initializing glove Embedding...')\n",
        "        \n",
        "        glove_weight=torch.from_numpy(np.load(os.path.join(self.tasks[0],'glove_embedding.npy')))\n",
        "        #glove_weight=torch.from_numpy(np.load('./glove_embedding.npy'))\n",
        "        self.emb.weight.data[:self.ntoken]=glove_weight\n",
        "  \n",
        "    def forward(self,x):\n",
        "        emb=self.emb(x)\n",
        "        emb=self.dropout(emb)\n",
        "        return emb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rQzMfjtJ2A06"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "from torch.nn.utils.weight_norm import weight_norm\n",
        "\n",
        "class SimpleClassifier(nn.Module):\n",
        "    def __init__(self,in_dim,hid_dim,out_dim,dropout):\n",
        "        super(SimpleClassifier,self).__init__()\n",
        "        layer=[\n",
        "            weight_norm(nn.Linear(in_dim,hid_dim),dim=None),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout,inplace=True),\n",
        "            weight_norm(nn.Linear(hid_dim,out_dim),dim=None)\n",
        "        ]\n",
        "        self.main=nn.Sequential(*layer)\n",
        "        \n",
        "    def forward(self,x):\n",
        "        logits=self.main(x)\n",
        "        return logits\n",
        "    \n",
        "    \n",
        "class SingleClassifier(nn.Module):\n",
        "    def __init__(self,in_dim,out_dim,dropout):\n",
        "        super(SingleClassifier,self).__init__()\n",
        "        layer=[\n",
        "            weight_norm(nn.Linear(in_dim,out_dim),dim=None),\n",
        "            nn.Dropout(dropout,inplace=True)\n",
        "        ]\n",
        "        self.main=nn.Sequential(*layer)\n",
        "        \n",
        "    def forward(self,x):\n",
        "        logits=self.main(x)\n",
        "        return logits   \n",
        "\n",
        "class FC(nn.Module):\n",
        "    def _init_(self,in_dim,out_dim,dropout):\n",
        "        super(FC,self).__init__()\n",
        "        self.main = nn.Linear(in_dim,out_dim)\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "    def forward(self, x):\n",
        "        logits = self.main(x)\n",
        "        logits = self.drop(logits)\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gcHnE_j62INr"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import h5py\n",
        "import pickle as pkl\n",
        "import json\n",
        "import random\n",
        "from transformers import get_linear_schedule_with_warmup,AdamW\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.utils.multiclass import type_of_target\n",
        "from sklearn.metrics import f1_score,recall_score,precision_score,accuracy_score,classification_report,precision_recall_fscore_support,roc_auc_score\n",
        "\n",
        "\n",
        "def log_hyperpara(logger,opt):\n",
        "    dic = vars(opt)\n",
        "    for k,v in dic.items():\n",
        "        logger.write(k + ' : ' + str(v))\n",
        "\n",
        "def bce_for_loss(logits,labels):\n",
        "    loss=nn.functional.binary_cross_entropy_with_logits(logits, labels)\n",
        "    # loss*=labels.size(1)\n",
        "    #print (loss)\n",
        "    return loss\n",
        "\n",
        "def compute_auc(logits,labels):\n",
        "    result=roc_auc_score(labels.cpu().numpy(),logits.cpu().numpy(),average='weighted')\n",
        "    return result\n",
        "\n",
        "def compute_score(logits,labels):\n",
        "    logits=torch.max(logits,1)[1]\n",
        "    labels=torch.max(labels,1)[1]\n",
        "    score=logits.eq(labels)\n",
        "    score=score.sum().float()\n",
        "    return score\n",
        "\n",
        "def compute_other(logits,labels,source=None):\n",
        "    acc=compute_score(logits,labels)\n",
        "    logits=np.argmax(logits.cpu().numpy(),axis=1)\n",
        "    label=np.argmax(labels.cpu().numpy(),axis=1)\n",
        "    length=logits.shape[0]\n",
        "\n",
        "    f1=f1_score(label,logits,average='weighted',labels=np.unique(label))\n",
        "    recall=recall_score(label,logits,average='weighted',labels=np.unique(label))\n",
        "    precision=precision_score(label,logits,average='weighted',labels=np.unique(label))\n",
        "\n",
        "    result=classification_report(label,logits)\n",
        "    #if source in [\"wz\",\"dt\",\"founta\"]:\n",
        "     #   print (result)\n",
        "    information=result.split('\\n')\n",
        "    #print(information,result)\n",
        "    cur=information[2].split('     ')\n",
        "    h_p=float(cur[3].strip())\n",
        "    h_r=float(cur[4].strip())\n",
        "    h_f=float(cur[5].strip())\n",
        "    total=[]\n",
        "    \n",
        "    total.append(precision*100)\n",
        "    total.append(recall*100)\n",
        "    total.append(f1*100)\n",
        "    total.append(h_p*100)\n",
        "    total.append(h_r*100)\n",
        "    total.append(h_f*100)\n",
        "    return total\n",
        "\n",
        "\n",
        "def setup_optimizer(model,mtl_loss):\n",
        "    exclude = \"s_rnn\"\n",
        "    exclude_2 = \"proj\"\n",
        "    bert_params = list(filter(lambda kv:kv[0] if kv[0].startswith(exclude) or kv[0].startswith(exclude_2) else None, model.named_parameters()))\n",
        "    other_params = list(filter(lambda kv:kv[0] if not kv[0].startswith(exclude) and not kv[0].startswith(exclude_2) else None, model.named_parameters()))\n",
        "                 \n",
        "    bert_params_value= []\n",
        "    other_params_value = []\n",
        "    for name, value in bert_params:\n",
        "        bert_params_value.append(value)\n",
        "    for name, value in other_params:\n",
        "        other_params_value.append(value)\n",
        "    for name, value in mtl_loss.named_parameters():\n",
        "        other_params_value.append(value)\n",
        "\n",
        "    \n",
        "    optimizer = torch.optim.AdamW([{\"params\":bert_params_value[:100], \"lr\":2e-5}, {\"params\":bert_params_value[100:], \"lr\":5e-5}, {\"params\":other_params_value}], lr=1e-3, eps=1e-9)\n",
        "    return optimizer\n",
        "\n",
        "def freeze(model):\n",
        "    exclude = \"s_rnn\"\n",
        "    exclude_2 = \"proj\"\n",
        "    bert_params = list(filter(lambda kv:kv[0] if kv[0].startswith(exclude) or kv[0].startswith(exclude_2) else None, model.named_parameters()))\n",
        "                \n",
        "    for name, value in bert_params:\n",
        "        value.requires_grad = False\n",
        "\n",
        "\n",
        "class MultiTaskLoss(nn.Module):\n",
        "    def __init__(self, tasks):\n",
        "        super(MultiTaskLoss, self).__init__()\n",
        "        self.tasks = tasks\n",
        "        self.log_vars = nn.Parameter(torch.zeros(len(self.tasks)).cuda(), requires_grad=True)\n",
        "\n",
        "    def forward(self, preds, targets, task_idx):\n",
        "        loss = bce_for_loss(preds, targets)\n",
        "        precision_0 = torch.exp(-self.log_vars[task_idx])\n",
        "        loss_ = precision_0 * loss\n",
        "        return loss_\n",
        "\n",
        "def train_for_deep(opt,model,total_train,total_test,total_val):\n",
        "    tasks=opt.TASKS.split(',')\n",
        "    #total_iters=[total_train[i].num_iters for i in range(len(tasks))]\n",
        "    total_iters=[total_train[i].num_iters for i in range(len(tasks))]\n",
        "    max_iters=sum(total_iters)\n",
        "    if tasks[0]=='dt':\n",
        "        logger=utils.Logger(os.path.join(opt.DT,'log'+str(opt.SAVE_NUM)+'.txt'))\n",
        "    elif tasks[0]=='wz':\n",
        "        logger=utils.Logger(os.path.join(opt.WZ_RESULT,'log'+str(opt.SAVE_NUM)+'.txt'))\n",
        "    elif tasks[0]=='founta':\n",
        "        logger=utils.Logger(os.path.join(opt.FOUNTA_RESULT,'log'+str(opt.SAVE_NUM)+'.txt'))\n",
        "    log_hyperpara(logger,opt)\n",
        "\n",
        "    mtl_loss=MultiTaskLoss(tasks)\n",
        "    \"\"\"\n",
        "    all tasks have the same number of training epochs\n",
        "    now for evaluating on other tasks, I just wanna to \n",
        "\n",
        "    WARNING!!!! swith optimizer to setup_optimizer() for angrybert\n",
        "    \"\"\"\n",
        "    #total_loss=0.0\n",
        "    \n",
        "    if opt.MODEL in ['angrybert','angrybert-attn','shared-bert']:\n",
        "        optimizer = setup_optimizer(model, mtl_loss)\n",
        "    else:\n",
        "        optimizer= torch.optim.AdamW(list(model.parameters())+list(mtl_loss.parameters()),lr=1e-3,eps=1e-8)\n",
        "\n",
        "    scheduler=get_linear_schedule_with_warmup(optimizer,\n",
        "                                              num_warmup_steps=0,\n",
        "                                              num_training_steps=max_iters*opt.EPOCHS\n",
        "                                             )\n",
        "    best_on_epoch = None\n",
        "    f1_max = 0\n",
        "    for epoch in range(opt.EPOCHS):\n",
        "        total_loss = [0.0 for _ in range(len(tasks))]\n",
        "        cur_iters = [0 for _ in range(len(tasks))]\n",
        "        # task to task index\n",
        "        cur_tasks = {task: i for i, task in enumerate(tasks)}\n",
        "        for iters in range(max_iters):\n",
        "            \"\"\"\n",
        "            choose a task among the remaining tasks\n",
        "            \"\"\"\n",
        "            choices=[cur_tasks[name] for name in cur_tasks.keys()]\n",
        "            task_idx=random.choice(choices)\n",
        "            batch_info=total_train[task_idx].next_batch()\n",
        "            tokens=batch_info['tokens'].cuda()\n",
        "            bert_tokens=batch_info['bert_tokens'].cuda()\n",
        "            labels=batch_info['label'].float().cuda()\n",
        "            masks=batch_info['masks'].cuda()\n",
        "            att_masks=batch_info['att_masks'].cuda()\n",
        "            pred,_=model(tokens,task_idx,bert_tokens,masks,att_masks)\n",
        "            loss=mtl_loss(pred,labels,task_idx)\n",
        "            total_loss[task_idx]+=loss\n",
        "            loss.backward()\n",
        "            nn.utils.clip_grad_norm_(model.parameters(),1.0)\n",
        "            optimizer.step()\n",
        "            scheduler.step()#updating the learning rate\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            cur_iters[task_idx]+=1\n",
        "            upper_bound = total_iters[task_idx]\n",
        "            if cur_iters[task_idx]>=upper_bound:\n",
        "                cur_tasks.pop(tasks[task_idx])\n",
        "\n",
        "        logger.write('epoch %d' %(epoch))\n",
        "        ttl_loss=np.sum(total_loss)\n",
        "        logger.write('\\t multi task train_loss: %.2f' % (ttl_loss))\n",
        "        #re-initialize the records for loss\n",
        "        total=[]\n",
        "        print ('Evaluating on each task')\n",
        "        #for basic MTL, roc is the F1 for the lingo task\n",
        "        model.train(False)\n",
        "        for j,task in enumerate(tasks):\n",
        "            print(j, task)\n",
        "            cur_total=evaluate_for_offensive(opt,model,total_test[j],j)\n",
        "            total.append(cur_total)\n",
        "            if j == 0:\n",
        "                eval_score=evaluate_for_offensive(opt,model,total_val,j)\n",
        "                f1_cur = eval_score[2]\n",
        "                \n",
        "                if f1_cur >= f1_max:\n",
        "                    f1_max = f1_cur\n",
        "                    best_on_epoch = cur_total\n",
        "\n",
        "                    print('\\tVALIDATION task %s precision: %.2f recall: %.2f f1: %.2f' % (task, eval_score[0], eval_score[1], eval_score[2]))\n",
        "            if task in ['dt','founta']:\n",
        "                logger.write('\\teval task %s precision: %.2f recall: %.2f f1: %.2f' % (task, cur_total[0], cur_total[1], cur_total[2]))\n",
        "                logger.write('\\teval task %s hate precision: %.2f recall: %.2f f1: %.2f\\n' % (task, cur_total[3],cur_total[4],cur_total[5]))\n",
        "            elif task == 'semeval_a':\n",
        "                logger.write('\\teval task %s roc auc score for multi label classification: %.2f \\n' % (task,  cur_total[0]))\n",
        "            else:\n",
        "                logger.write('\\teval task %s precision: %.2f recall: %.2f f1: %.2f' % (task, cur_total[0], cur_total[1], cur_total[2]))\n",
        "        model.train(True)\n",
        "    total[0] = best_on_epoch\n",
        "    return total\n",
        "\n",
        "def evaluate_for_offensive(opt,model,test_set,task_idx):\n",
        "    # print ('The information for task 1 iterations is:',len(test_set),test_set.last_batch)\n",
        "    task=opt.TASKS.split(',')[task_idx]\n",
        "    total = len(test_set)\n",
        "    for i in range(total):\n",
        "        with torch.no_grad():\n",
        "            batch_info=test_set.next_batch()\n",
        "            tokens=batch_info['tokens'].cuda()\n",
        "            bert_tokens=batch_info['bert_tokens'].cuda()\n",
        "            labels=batch_info['label'].float().cuda()\n",
        "            masks=batch_info['masks'].cuda()\n",
        "            att_masks=batch_info['att_masks'].cuda()\n",
        "            pred,_=model(tokens,task_idx,bert_tokens,masks,att_masks)\n",
        "\n",
        "        if i==0:\n",
        "            t1_labels=labels\n",
        "            t1_pred=pred\n",
        "        else:\n",
        "            t1_labels=torch.cat((t1_labels,labels),0)\n",
        "            t1_pred=torch.cat((t1_pred,pred),0)\n",
        "    total=compute_other(t1_pred,t1_labels,task)\n",
        "    if task == \"semeval_a\":\n",
        "        total[0]=compute_auc(t1_pred,t1_labels)*100\n",
        "    return total\n",
        "\n",
        "\n",
        "def analysis_dump(opt,model,test_set,idx):\n",
        "    tasks = opt.TASKS.split(',')\n",
        "    total = len(test_set)\n",
        "\n",
        "    result = dict()\n",
        "    for task in tasks:\n",
        "        result[task] = {\"weights\":[], \"preds\":[]}\n",
        "        result[\"words\"] = []\n",
        "        result[\"labels\"] = []\n",
        "    for i in range(total):\n",
        "        with torch.no_grad():\n",
        "            batch_info = test_set.next_batch()\n",
        "            tokens = batch_info['tokens'].cuda()\n",
        "            labels = batch_info['label'].float().cuda()\n",
        "            bert_tokens = batch_info['bert_tokens'].cuda()\n",
        "            masks = batch_info['masks'].cuda()\n",
        "            att_masks = batch_info['att_masks'].cuda()\n",
        "            words = batch_info['words']\n",
        "            result[\"words\"].extend(words)\n",
        "            result[\"labels\"].extend(labels.cpu().numpy())\n",
        "            for task_idx, task in enumerate(tasks):\n",
        "                pred, _ = model(tokens, task_idx, bert_tokens, masks, att_masks)\n",
        "                pred = pred.cpu().numpy()\n",
        "                result[task][\"preds\"].extend(pred)\n",
        "\n",
        "    write_json(result, tasks[0],idx)\n",
        "\n",
        "def write_json(result, source, idx):\n",
        "    with open(\"weights/{}.mtddn.analysis.json\".format(source+str(idx)), \"w\") as f:\n",
        "        json.dump(result, f, cls=NpEncoder)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mS8GQlkF2QD7"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "from torch.nn.utils.weight_norm import weight_norm\n",
        "\n",
        "class FCNet(nn.Module):\n",
        "    def __init__(self,in_dim,out_dim,dropout):\n",
        "        super(FCNet,self).__init__()\n",
        "        self.in_dim=in_dim\n",
        "        self.out_dim=out_dim\n",
        "        self.relu=nn.ReLU()\n",
        "        self.linear=weight_norm(nn.Linear(in_dim,out_dim),dim=None)\n",
        "        self.dropout=nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self,x):\n",
        "        logits=self.dropout(self.linear(x))\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C02h7U5S2M4F"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self,opt):\n",
        "        super(Attention,self).__init__()\n",
        "        self.opt=opt\n",
        "        self.v_proj=FCNet(self.opt.NUM_HIDDEN,self.opt.PROJ_DIM,self.opt.FC_DROPOUT)\n",
        "        self.q_proj=FCNet(self.opt.NUM_HIDDEN,self.opt.PROJ_DIM,self.opt.FC_DROPOUT)\n",
        "        self.att=FCNet(self.opt.PROJ_DIM,1,self.opt.FC_DROPOUT)\n",
        "        self.softmax=nn.Softmax()\n",
        "        \n",
        "    def forward(self,v,q):\n",
        "        v_proj=self.v_proj(v)\n",
        "        q_proj=torch.unsqueeze(self.q_proj(q),1)\n",
        "        vq_proj=F.relu(v_proj +q_proj)\n",
        "        proj=torch.squeeze(self.att(vq_proj))\n",
        "        w_att=torch.unsqueeze(self.softmax(proj),2)\n",
        "        vatt=v * w_att\n",
        "        att=torch.sum(vatt,1)\n",
        "        return att\n",
        "\n",
        "class SelfAttention(nn.Module):\n",
        "  def __init__(self, query_dim, key_dim, value_dim):\n",
        "    super(SelfAttention, self).__init__()\n",
        "    self.scale = 1. / math.sqrt(query_dim)\n",
        "\n",
        "  def forward(self, query, keys, values, masks=None):\n",
        "    # Query = [BxQ]\n",
        "    # Keys = [TxBxK]\n",
        "    # Values = [TxBxV]\n",
        "    # Outputs = a:[TxB], lin_comb:[BxV]\n",
        "\n",
        "    # Here we assume q_dim == k_dim (dot product attention)\n",
        "    \n",
        "    query = query.unsqueeze(1) # [BxQ] -> [Bx1xQ]\n",
        "    keys = keys.transpose(1,2) # [TxBxK] -> [BxKxT]\n",
        "    energy = torch.bmm(query, keys) # [Bx1xQ]x[BxKxT] -> [Bx1xT]\n",
        "    masks = (1.0 - masks) * -10000.0\n",
        "    masks = masks.unsqueeze(1)\n",
        "    energy = energy + masks\n",
        "    energy = F.softmax(energy.mul_(self.scale), dim=2) # scale, normalize\n",
        "    # values = values.transpose(0,1) # [TxBxV] -> [BxTxV]\n",
        "    linear_combination = torch.bmm(energy, values).squeeze(1) #[Bx1xT]x[BxTxV] -> [BxV]\n",
        "    energy = energy.squeeze(1)\n",
        "    return linear_combination, energy\n",
        "\n",
        "\n",
        "class Bilinear_Att(nn.Module):\n",
        "    def __init__(self,in_a,in_b,bilinear_dim,dropout):\n",
        "        super(Bilinear_Att,self).__init__()\n",
        "        self.proj_a=SingleClassifier(in_a,bilinear_dim,dropout)\n",
        "        self.proj_b=SingleClassifier(in_b,bilinear_dim,dropout)\n",
        "        self.proj=SingleClassifier(bilinear_dim,1,dropout)\n",
        "        self.softmax=nn.Softmax(dim=1)\n",
        "        \n",
        "        \n",
        "    def forward(self,a,b):\n",
        "        proj_a=self.proj_a(a)\n",
        "        proj_b=self.proj_b(b)\n",
        "        modi_a=proj_a.transpose(1,2).unsqueeze(3)\n",
        "        modi_b=proj_b.transpose(1,2).unsqueeze(2)\n",
        "        final=torch.matmul(modi_a,modi_b).transpose(1,2).transpose(2,3)\n",
        "        final=torch.squeeze(self.proj(final)) #B* N_a * N_b\n",
        "        modi=final.view(-1,a.size()[1]*b.size()[1]).contiguous()\n",
        "        norm_weight=self.softmax(modi).view(-1,a.size()[1],b.size()[1]).contiguous()\n",
        "        return norm_weight\n",
        "    \n",
        "class MFB(nn.Module):\n",
        "    '''this version does not add nolinear layer'''\n",
        "    def __init__(self,opt):\n",
        "        super(MFB,self).__init__()    \n",
        "        self.proj_x=nn.Linear(opt.NUM_HIDDEN,opt.NUM_HIDDEN*3)\n",
        "        self.proj_y=nn.Linear(opt.NUM_HIDDEN+5,opt.NUM_HIDDEN*3)\n",
        "        self.dropout=nn.Dropout(opt.FC_DROPOUT)\n",
        "        self.opt=opt\n",
        "        self.final_proj=nn.Linear(opt.NUM_HIDDEN,opt.NUM_HIDDEN)\n",
        "        \n",
        "        \n",
        "    def forward(self,x,y):\n",
        "        batch_size=x.shape[0]\n",
        "        proj_x=self.dropout(self.proj_x(x))\n",
        "        proj_y=self.dropout(self.proj_y(y))\n",
        "        xy=proj_x * proj_y #B,3H\n",
        "        reshape_xy=xy.view(batch_size,self.opt.NUM_HIDDEN,-1)\n",
        "        pool_xy=torch.sum(reshape_xy,dim=2)\n",
        "        final_xy=self.final_proj(pool_xy)\n",
        "        sqrt_xy=torch.sqrt(F.relu(final_xy))-torch.sqrt(F.relu(-final_xy))\n",
        "        norm_xy=F.normalize(sqrt_xy)\n",
        "        return norm_xy\n",
        "\n",
        "class Intra(nn.Module):\n",
        "    '''this version does not add nolinear layer'''\n",
        "    def __init__(self,opt):\n",
        "        super(Intra,self).__init__()\n",
        "        self.opt=opt\n",
        "        self.softmax=nn.Softmax(dim=2)\n",
        "        self.proj=nn.Linear(opt.NUM_HIDDEN * 2,opt.NUM_HIDDEN)\n",
        "        \n",
        "        \n",
        "    '''considering the impact of b'''    \n",
        "    def forward(self,a,b):\n",
        "        simi_matrix=torch.bmm(a,b.transpose(1,2))/math.sqrt(self.opt.NUM_HIDDEN)# B*dim_a*dim_b\n",
        "        norm_weight=self.softmax(simi_matrix)\n",
        "        up_1=torch.bmm(norm_weight,b) + a\n",
        "        return up_1\n",
        "\n",
        "\n",
        "class Gate_Attention(nn.Module):\n",
        "    def __init__(self,num_hidden_a,num_hidden_b,num_hidden):\n",
        "        super(Gate_Attention,self).__init__()\n",
        "        self.hidden=num_hidden\n",
        "        self.w1=nn.Parameter(torch.Tensor(num_hidden_a,num_hidden))\n",
        "        self.w2=nn.Parameter(torch.Tensor(num_hidden_b,num_hidden))\n",
        "        self.bias=nn.Parameter(torch.Tensor(num_hidden))\n",
        "        self.reset_parameter()\n",
        "        \n",
        "    def reset_parameter(self):\n",
        "        stdv1=1. / math.sqrt(self.hidden)\n",
        "        stdv2=1. / math.sqrt(self.hidden)\n",
        "        stdv= (stdv1 + stdv2) / 2.\n",
        "        self.w1.data.uniform_(-stdv1,stdv1)\n",
        "        self.w2.data.uniform_(-stdv2,stdv2)\n",
        "        self.bias.data.uniform_(-stdv,stdv)\n",
        "        \n",
        "    def forward(self,a,b):\n",
        "        wa=torch.matmul(a,self.w1)\n",
        "        wb=torch.matmul(b,self.w2)\n",
        "        gated=wa+wb+self.bias\n",
        "        gate=torch.sigmoid(gated)\n",
        "        output=gate * a + (1-gate) * b\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p7DoCmmf10AI"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn.utils.rnn import pack_padded_sequence\n",
        "from torch.autograd import Function\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import copy\n",
        "from transformers import BertForSequenceClassification,BertConfig\n",
        "\n",
        "def clones(module,N):\n",
        "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n",
        "\n",
        "\"\"\"\n",
        "all previous implementations have been removed \n",
        "(since our previous operation is for two inputs in the forward function)\n",
        "if wanna to check \n",
        "please refer to previous codes\n",
        "\"\"\"\n",
        "\n",
        "class Joint_Multi(nn.Module):\n",
        "    def __init__(self,shared_emb,private_emb,shared_rnn,private_rnn,gates,fcs,mode,opt,atts=None,rnn_last=None):\n",
        "        super(Joint_Multi,self).__init__()\n",
        "        self.w_emb=shared_emb\n",
        "        self.embs=private_emb\n",
        "        self.s_rnn=shared_rnn\n",
        "        self.p_rnn=private_rnn\n",
        "        self.gates=gates\n",
        "        self.fcs=fcs\n",
        "        self.atts = atts\n",
        "        self.mode=mode\n",
        "        self.rnn_last = rnn_last\n",
        "        self.drop = nn.Dropout(0.20)\n",
        "    def forward(self,text,task_idx=0,bert_tokens=None,masks=None,att_masks=None): \n",
        "        shared_emb=self.w_emb(text)\n",
        "        private_emb=self.embs(text)\n",
        "        \n",
        "        lens = (att_masks != 0).sum(dim=1).cuda()\n",
        "\n",
        "        packed_input = pack_padded_sequence(private_emb, lengths=lens, batch_first=True)\n",
        "        s_packed_input = pack_padded_sequence(shared_emb, lengths=lens, batch_first=True)\n",
        "        s_out, shared_rnn=self.s_rnn(s_packed_input)\n",
        "\n",
        "        if task_idx == 0:\n",
        "            p_out, private_rnn=self.p_rnn(packed_input)  \n",
        "            p_out = self.drop(p_out)\n",
        "            gated_private_rnn = p_out*0.1 + s_out*0.9\n",
        "            gated_private_rnn = pack_padded_sequence(gated_private_rnn, lengths=lens, batch_first=True)\n",
        "            p_out, rnn_o = self.rnn_last(gated_private_rnn)\n",
        "        else: \n",
        "            s_out = self.drop(s_out)\n",
        "            s_out = pack_padded_sequence(s_out, lengths=lens, batch_first=True)\n",
        "            p_out, rnn_o = self.rnn_last(s_out)\n",
        "        result, _ = self.atts[task_idx](rnn_o, p_out, p_out, att_masks)\n",
        "\n",
        "        logits=self.fcs[task_idx](result)\n",
        "        return logits, _\n",
        "\n",
        "\n",
        "class Deep_Multi(nn.Module):\n",
        "    def __init__(self,shared_emb,private_emb,shared_rnn,private_rnn,gates,fcs,mode,opt,atts=None):\n",
        "        super(Deep_Multi,self).__init__()\n",
        "        self.w_emb=shared_emb\n",
        "        self.embs=private_emb\n",
        "        self.s_rnn=shared_rnn\n",
        "        self.rnns=private_rnn\n",
        "        self.gates=gates\n",
        "        self.fcs=fcs\n",
        "        self.atts = atts\n",
        "        self.proj=nn.Linear(768,opt.NUM_HIDDEN*2)\n",
        "        self.mode=mode \n",
        "        \n",
        "    def forward(self,text,task_idx=0,bert_tokens=None,masks=None,att_masks=None):\n",
        "        weights = None\n",
        "        shared_emb=self.w_emb(text)\n",
        "        private_emb=self.embs[task_idx](text)\n",
        "        if self.mode in ['gate','dnn','cnn','uniform']:\n",
        "            embedding=torch.cat((shared_emb,private_emb),dim=2)\n",
        "        elif self.mode in ['basic','angrybert','angrybert-attn']:\n",
        "            embedding = private_emb\n",
        "        \n",
        "        if self.mode=='cnn':\n",
        "            shared_rnn=self.s_rnn(embedding)\n",
        "            private_rnn=self.rnns[task_idx](embedding)\n",
        "\n",
        "        elif self.mode == 'shared-bert':\n",
        "            shared_rnn = self.s_rnn(bert_tokens, token_type_ids=None, attention_mask=masks)\n",
        "            shared_rnn = shared_rnn[1][-1][:, 0, :]\n",
        "\n",
        "            result = self.proj(shared_rnn)\n",
        "\n",
        "        elif self.mode=='angrybert':\n",
        "            shared_rnn=self.s_rnn(bert_tokens,token_type_ids=None,attention_mask=masks)\n",
        "            shared_rnn=shared_rnn[1][-1][:,0,:]\n",
        "            \n",
        "            shared_rnn=self.proj(shared_rnn)\n",
        "            shared_rnn=F.relu(shared_rnn)\n",
        "            lens = (att_masks != 0).sum(dim=1).cuda()\n",
        "            packed_input = pack_padded_sequence(embedding, lengths=lens, batch_first=True)\n",
        "            _,private_rnn=self.rnns[task_idx](packed_input)\n",
        "\n",
        "        elif self.mode=='angrybert-attn':\n",
        "            shared_rnn=self.s_rnn(bert_tokens,token_type_ids=None,attention_mask=masks)\n",
        "            shared_rnn=shared_rnn[1][-1][:,0,:]\n",
        "\n",
        "            shared_rnn=self.proj(shared_rnn)\n",
        "            lens = (att_masks != 0).sum(dim=1).cuda()\n",
        "            packed_input = pack_padded_sequence(embedding, lengths=lens, batch_first=True)\n",
        "            p_out,private_rnn=self.rnns[task_idx](packed_input)\n",
        "            p_attn, weights = self.atts[task_idx](private_rnn, p_out, p_out, att_masks)\n",
        "            result = self.gates[task_idx](shared_rnn, p_attn)\n",
        "        else:\n",
        "            lens = (att_masks != 0).sum(dim=1).cuda()\n",
        "            packed_input = pack_padded_sequence(embedding, lengths=lens, batch_first=True)\n",
        "            _,shared_rnn=self.s_rnn(packed_input)\n",
        "            _,private_rnn=self.rnns[task_idx](packed_input)\n",
        "        \n",
        "\n",
        "        if self.mode in ['gate','angrybert']:\n",
        "            result=self.gates[task_idx](shared_rnn,private_rnn)\n",
        "        elif self.mode=='basic':\n",
        "            result=torch.cat((shared_rnn,private_rnn),dim=1)\n",
        "        elif self.mode in ['dnn','cnn']:\n",
        "            result=private_rnn\n",
        "        elif self.mode=='uniform':\n",
        "            result=shared_rnn\n",
        "        logits=self.fcs[task_idx](result)\n",
        "        return logits, weights\n",
        "    \n",
        "class Deep_Coupled(nn.Module):\n",
        "    def __init__(self,shared_emb,private_emb,couple,fcs):\n",
        "        super(Deep_Coupled,self).__init__()\n",
        "        self.embs=private_emb\n",
        "        self.w_emb=shared_emb #not used\n",
        "        self.couple=couple\n",
        "        self.fcs=fcs\n",
        "\n",
        "    def forward(self,text,task_idx=0,bert_tokens=None,masks=None,att_masks=None):\n",
        "        w_emb=self.embs[task_idx](text)\n",
        "        result,_=self.couple(w_emb,task_idx)\n",
        "        logits=self.fcs[task_idx](result)\n",
        "        return logits, _\n",
        "    \n",
        "def build_baseline(dataset,opt):\n",
        "    \"\"\"\n",
        "    what models do we provide in the baseline part:\n",
        "    basic: shared and private rnns, concatenation of both to the fc layer\n",
        "    dnn: shared and private embeddings, private rnn\n",
        "    cnn: shared and private embeddings, private cnn\n",
        "    uniform: different embeddings, shared rnnm, 2016 IJCAI first baseline\n",
        "    local: 2016 IJCAI, shared layer in the paper\n",
        "    sp-mtl: 2017 IJCAI, for the implementation of global fusion\n",
        "    mtl-gatedencoder: joint modeling network, Rajamanickam et al. 20\n",
        "    shared-bert: bert baseline model for multitask shared learning\n",
        "    angrybert: AngryBERT model, this is our proposed model\n",
        "    angrybert-attention: AngryBERT model with attention on top, visualization purpose\n",
        "    \"\"\"\n",
        "    opt= parse_opt()\n",
        "    mode=opt.MODEL\n",
        "    em_times=1\n",
        "    fc_times=1\n",
        "    fin_times = 1\n",
        "\n",
        "    if mode in ['gate','dnn','cnn','uniform']:\n",
        "        em_times=2\n",
        "    if mode in ['basic', 'angrybert','angrybert-attn', 'shared-bert']:\n",
        "        fc_times=2\n",
        "\n",
        "    if mode in [\"dnn\", \"uniform\", \"angrybert\", \"angrybert-attn\", 'shared-bert', \"mtl-gatedencoder\"]:\n",
        "        fin_times = 2\n",
        "    elif mode  == \"basic\":\n",
        "        fin_times = 4\n",
        "\n",
        "    datasets=opt.TASKS.split(',')\n",
        "    num_tasks=len(datasets)\n",
        "    fcs=nn.ModuleList()\n",
        "    task2dim={'wz':3,'dt':3,'founta':4,'hatelingo':5,'offenseval_c':3,'semeval_a':11}\n",
        "    final_dim=opt.NUM_HIDDEN\n",
        "    \n",
        "    if opt.MODEL=='cnn':\n",
        "        final_dim=len(opt.FILTER_SIZE.split(',')*opt.NUM_FILTER)\n",
        "    for task in datasets:\n",
        "        dim=task2dim[task]\n",
        "        fc=SimpleClassifier(fin_times*final_dim,opt.MID_DIM,dim,opt.FC_DROPOUT).cuda()\n",
        "        fcs.append(fc)\n",
        "\n",
        "    if mode=='cnn':\n",
        "        #this is actually cnn but named as rnn to arrange the code\n",
        "        shared_rnn=CNN_Model(em_times*opt.EMB_DIM,opt.FILTER_SIZE,opt.NUM_FILTER)\n",
        "        private_rnn=clones(CNN_Model(em_times*opt.EMB_DIM,opt.FILTER_SIZE,opt.NUM_FILTER),num_tasks)\n",
        "    else:\n",
        "        shared_rnn=BiLSTMPacked(em_times*opt.EMB_DIM,opt.NUM_HIDDEN,opt.NUM_LAYER,opt.BIDIRECT,opt.L_RNN_DROPOUT)\n",
        "        private_rnn=clones(BiLSTMPacked(em_times*opt.EMB_DIM,opt.NUM_HIDDEN,opt.NUM_LAYER,opt.BIDIRECT,opt.L_RNN_DROPOUT),num_tasks)\n",
        "    \n",
        "    shared_emb=Word_Embedding(dataset.dictionary.ntoken(),opt.EMB_DIM,opt.EMB_DROPOUT) \n",
        "    private_emb=clones(Word_Embedding(dataset.dictionary.ntoken(),opt.EMB_DIM,opt.EMB_DROPOUT),num_tasks)\n",
        "    \n",
        "    if mode in ['uniform', 'cnn', 'dnn', 'basic', 'angrybert','angrybert-attn', 'shared-bert']:\n",
        "        gates=clones(Gate_Attention(opt.NUM_HIDDEN*fc_times,opt.NUM_HIDDEN*fc_times,opt.NUM_HIDDEN*fc_times).cuda(),num_tasks)\n",
        "        atts = clones(SelfAttention(fc_times*opt.NUM_HIDDEN,fc_times*opt.NUM_HIDDEN,fc_times*opt.NUM_HIDDEN), num_tasks)   \n",
        "        if mode in ['angrybert','angrybert-attn','shared-bert']:\n",
        "            shared_rnn=model=BertForSequenceClassification.from_pretrained(\n",
        "                'bert-base-uncased',\n",
        "                num_labels=num_tasks,\n",
        "                output_attentions=False,\n",
        "                output_hidden_states=True\n",
        "            )\n",
        "        return Deep_Multi(shared_emb,private_emb,shared_rnn,private_rnn,gates,fcs,mode,opt,atts)\n",
        "    elif mode==\"mtl-gatedencoder\":\n",
        "        private_emb=Word_Embedding(dataset.dictionary.ntoken(),opt.EMB_DIM,opt.EMB_DROPOUT)\n",
        "        gates=Gate_Attention(opt.NUM_HIDDEN*2,opt.NUM_HIDDEN*2,opt.NUM_HIDDEN*2).cuda()\n",
        "        atts = clones(SelfAttention(fc_times*opt.NUM_HIDDEN,fc_times*opt.NUM_HIDDEN,fc_times*opt.NUM_HIDDEN), num_tasks)   \n",
        "        private_rnn = BiLSTMPacked(em_times*opt.EMB_DIM,opt.NUM_HIDDEN,opt.NUM_LAYER,opt.BIDIRECT,opt.L_RNN_DROPOUT)\n",
        "        rnn_last =BiLSTMPacked(2*opt.NUM_HIDDEN,opt.NUM_HIDDEN,opt.NUM_LAYER,opt.BIDIRECT,opt.L_RNN_DROPOUT)\n",
        "        return Joint_Multi(shared_emb,private_emb,shared_rnn,private_rnn,gates,fcs,mode,opt,atts,rnn_last)\n",
        "\n",
        "    elif mode=='couple':\n",
        "        couple= Coupled_Layer(opt.EMB_DIM,opt.NUM_HIDDEN,num_tasks)\n",
        "        return Deep_Coupled(shared_emb,private_emb,couple,fcs)\n",
        "    elif mode=='local':\n",
        "        couple=Local_Layer(opt.EMB_DIM,opt.NUM_HIDDEN,num_tasks)\n",
        "        return Deep_Coupled(shared_emb,private_emb,couple,fcs)\n",
        "    elif mode=='sp-mtl':\n",
        "        gate=Gate_Attention(opt.NUM_HIDDEN*2,opt.NUM_HIDDEN,opt.NUM_HIDDEN)\n",
        "        couple=Shared_Layer(opt.EMB_DIM,opt.NUM_HIDDEN,num_tasks,gate)\n",
        "        return Deep_Coupled(shared_emb,private_emb,couple,fcs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rsMVX13OvJIZ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "import os\n",
        "import pickle as pkl\n",
        "\n",
        "if __name__=='__main__':\n",
        "    opt= parse_opt()\n",
        "    # torch.cuda.set_device(opt.CUDA_DEVICE)\n",
        "    torch.manual_seed(opt.SEED)\n",
        "    \n",
        "    tasks=opt.TASKS.split(',')\n",
        "    hate_dataset=tasks[0]\n",
        "    # result saving\n",
        "    if hate_dataset=='wz':\n",
        "        logger=Logger(os.path.join(opt.WZ_RESULT,'final_'+str(opt.SAVE_NUM)+'.txt'))\n",
        "    elif hate_dataset=='dt':\n",
        "        logger=Logger(os.path.join(opt.DT,'final_'+str(opt.SAVE_NUM)+'.txt'))\n",
        "    elif hate_dataset=='founta':\n",
        "        logger=Logger(os.path.join(opt.FOUNTA_RESULT,'final_'+str(opt.SAVE_NUM)+'.txt'))\n",
        "    \n",
        "    dictionary=Base_Op()\n",
        "    dictionary.init_dict()\n",
        "    hate_dataset=pkl.load(open(os.path.join(opt.SPLIT_DATASET,hate_dataset+'.pkl'),'rb'))\n",
        "    \n",
        "    constructor='build_baseline'\n",
        "    #definitions for criteria\n",
        "    \n",
        "    \"\"\"\n",
        "    recording for both total F1 and hate F1\n",
        "    change to dynamic number of tasks\n",
        "    \"\"\"\n",
        "    total=[]\n",
        "    for i in range(opt.CROSS_VAL):\n",
        "        \"\"\"\n",
        "        construct a list\n",
        "        the first position of the list is the hate speech detection dataset\n",
        "        \"\"\"\n",
        "        total_train=[]\n",
        "        total_test=[]\n",
        "        for task_idx,task in enumerate(tasks):\n",
        "            print (task_idx,task)\n",
        "            dataset=pkl.load(open(os.path.join(opt.SPLIT_DATASET,task)+'.pkl','rb'))\n",
        "            train_set=Wraped_Data(opt,dictionary,dataset,i, source=task)\n",
        "            test_set=Wraped_Data(opt,dictionary,dataset,i,'test', source=task)\n",
        "            total_train.append(train_set)\n",
        "            total_test.append(test_set)\n",
        "            if task_idx == 0:\n",
        "                val_set=Wraped_Data(opt,dictionary,dataset,opt.CROSS_VAL,'val', source=task)\n",
        "        model=build_baseline(total_test[0],opt)   \n",
        "        model=model.cuda()\n",
        "        model.w_emb.init_embedding()\n",
        "        #cur_total is the result for each task\n",
        "        cur_total=train_for_deep(opt,model,total_train,total_test,val_set)\n",
        "        \n",
        "        total.append(cur_total)\n",
        "\n",
        "        \"\"\"\n",
        "        please don't believe in it for WZ\n",
        "        since WZ has two hate classes\n",
        "        this is adaptive for DT and FOUNTA\n",
        "        \"\"\"\n",
        "        logger.write('validation folder %d' %(i+1))\n",
        "        for j,task in enumerate(tasks):\n",
        "            if task in ['dt','founta']:\n",
        "                logger.write('\\teval task %s precision: %.2f ' % (task, cur_total[j][0]))\n",
        "                logger.write('\\teval task %s recall: %.2f ' % (task, cur_total[j][1]))\n",
        "                logger.write('\\teval task %s f1: %.2f ' % (task, cur_total[j][2]))\n",
        "                logger.write('\\teval task %s hate precision: %.2f ' % (task, cur_total[j][3]))\n",
        "                logger.write('\\teval task %s hate recall: %.2f ' % (task,  cur_total[j][4]))\n",
        "                logger.write('\\teval task %s hate f1: %.2f \\n' % (task,  cur_total[j][5]))\n",
        "            elif task == 'semeval_a':\n",
        "                logger.write('\\teval task %s roc auc score for multi label classification: %.2f \\n' % (task,  cur_total[j][0]))\n",
        "            else:\n",
        "                logger.write('\\teval task %s precision: %.2f ' % (task, cur_total[j][0]))\n",
        "                logger.write('\\teval task %s recall: %.2f ' % (task, cur_total[j][1]))\n",
        "                logger.write('\\teval task %s f1: %.2f \\n' % (task, cur_total[j][2]))\n",
        "    \n",
        "    total=np.sum(total,axis=0)\n",
        "\n",
        "    logger.write('\\n final result')\n",
        "    \n",
        "    for j,task in enumerate(tasks):\n",
        "        if task in ['dt','founta']:\n",
        "            logger.write('\\teval task %s precision: %.2f ' % (task,total[j][0]/opt.CROSS_VAL))\n",
        "            logger.write('\\teval task %s recall: %.2f ' % (task, total[j][1]/opt.CROSS_VAL))\n",
        "            logger.write('\\teval task %s f1: %.2f ' % (task, total[j][2]/opt.CROSS_VAL))\n",
        "            logger.write('\\teval task %s hate precision: %.2f ' % (task, total[j][3]/opt.CROSS_VAL))\n",
        "            logger.write('\\teval task %s hate recall: %.2f ' % (task, total[j][4]/opt.CROSS_VAL))\n",
        "            logger.write('\\teval task %s hate f1: %.2f \\n' % (task, total[j][5]/opt.CROSS_VAL))\n",
        "        elif task  == 'semeval_a':\n",
        "            logger.write('\\teval task %s roc auc score for multi label classification: %.2f \\n' % (task, total[j][0]/opt.CROSS_VAL))\n",
        "        else:\n",
        "            logger.write('\\teval task %s precision: %.2f ' % (task,total[j][0]/opt.CROSS_VAL))\n",
        "            logger.write('\\teval task %s recall: %.2f ' % (task, total[j][1]/opt.CROSS_VAL))\n",
        "            logger.write('\\teval task %s f1: %.2f ' % (task, total[j][2]/opt.CROSS_VAL))\n",
        "    exit(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nhTxNtKZkljj"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "Aggression BERT",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}